{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8DHS6ZlhIuE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from pandas import DataFrame as df \n",
    "import numpy as np \n",
    "import scipy.stats as st \n",
    "import datetime as dt \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline \n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4r0Etnb4hIuK"
   },
   "outputs": [],
   "source": [
    "def logRet(df):\n",
    "    pri = df\n",
    "    daily_returns = pri * 0\n",
    "    for i in range(1, len(pri)-1):\n",
    "        daily_returns[i] = np.log(pri[i]/pri[i-1])\n",
    "    return daily_returns\n",
    "\n",
    "# For a portfolio's returns, gives the index\n",
    "def indx(ret):\n",
    "    ret = ret.reset_index(drop=True)\n",
    "    ind = ret * 0 + 100\n",
    "    for i in range(1, len(ret)-1):\n",
    "        ind[i] = ind[i-1] * (1+ret[i])\n",
    "    return ind\n",
    "\n",
    "# Cumulative returns\n",
    "\n",
    "def cumRet(rets):\n",
    "    cumR = rets * 0 \n",
    "    for t in range(1, len(cumR)-1):\n",
    "        cumR[t] = cumR[t-1] + rets[t]\n",
    "    return cumR\n",
    "\n",
    "# Risk metric functions\n",
    "\n",
    "def maxDrawdown(pri, window=252):\n",
    "    Roll_Max = pri.rolling(window, min_periods=200).max()\n",
    "    Daily_Drawdown = pri/Roll_Max - 1.0\n",
    "    Daily_Drawdown *= -1\n",
    "    Max_Daily_Drawdown = np.max(Daily_Drawdown)\n",
    "    # print(\"Max daily drawdown: {}%\".format(round(Max_Daily_Drawdown, 2)))\n",
    "    return Max_Daily_Drawdown\n",
    "\n",
    "def plotDrawdown(pri, window=252):\n",
    "    Roll_Max = pri.rolling(window, min_periods=200).max()\n",
    "    Daily_Drawdown = pri/Roll_Max - 1.0\n",
    "    Daily_Drawdown *= -1\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_axes([1,1,1,1])\n",
    "    ax.plot(Daily_Drawdown)\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('%')\n",
    "    ax.set_title('Drawdown')\n",
    "    plt.show()\n",
    "\n",
    "def valueAR(dailyRet, per=5): #Historical\n",
    "    pci = np.percentile(dailyRet, per)\n",
    "    # print(\"VaR at {0}%: {1}%\".format(per, round(-100*pci, 3)))\n",
    "    return -pci\n",
    "\n",
    "def cVaR(dailyRet, per=5):\n",
    "    pci = np.percentile(dailyRet, per)\n",
    "    sumBad = 0\n",
    "    t = 0\n",
    "    for ret in dailyRet:\n",
    "        if ret <= pci:\n",
    "            t += 1\n",
    "            sumBad += ret\n",
    "    if t > 0:\n",
    "        condVaR = sumBad / t\n",
    "    else:\n",
    "        condVaR = 0\n",
    "    # print(\"CVaR at {0}%: {1}%\".format(per, round(-100*condVaR, 3)))\n",
    "    return -condVaR\n",
    "\n",
    "def semiv(dayRet, port=False):\n",
    "    m = np.mean(dayRet)\n",
    "    low = []\n",
    "    for ret in dayRet:\n",
    "        if ret <= m:\n",
    "            low.append(ret)\n",
    "    stand = np.std(low)\n",
    "    if port: \n",
    "        stand *= np.sqrt(21)\n",
    "    # print(\"SemiSD: {}\".format(round(stand, 6)))\n",
    "    return stand\n",
    "\n",
    "# Use pricing data\n",
    "def max_drawdown(X):\n",
    "    mdd = 0\n",
    "    peak = X[0]\n",
    "    for x in X:\n",
    "        if x > peak:\n",
    "            peak = x\n",
    "        else:\n",
    "            dd = (peak - x) / peak\n",
    "            if dd > mdd:\n",
    "                mdd = dd\n",
    "    return mdd\n",
    "\n",
    "# Adjusted to use returns data \n",
    "def maxDDRet(rets):\n",
    "    return max_drawdown(indx(rets))\n",
    "\n",
    "# SD, SemiV, Drawdown, VaR, CVaR, Skewness, Kurtosis\n",
    "def riskMetrics(ret, name = \"portfolio\", varP = 5, cvarP = 5):\n",
    "    print(\"Risk metrics for {}\".format(name))\n",
    "    pri = indx(ret)\n",
    "    print(\"Average return: {}%\".format(round(100*np.mean(ret), 6)))\n",
    "    sd = np.std(ret)\n",
    "    print(\"SD: {}\".format(round(sd, 6)))\n",
    "    semiv(ret)\n",
    "    dd = max_drawdown(pri)\n",
    "    print(\"Max DD: {}%\".format(round(100*dd, 2)))\n",
    "    valueAR(ret, varP)\n",
    "    cVAR(ret, cvarP)\n",
    "    sk = st.skew(ret)\n",
    "    print(\"Skewness: {}\".format(round(sk, 6)))\n",
    "    kurt = st.kurtosis(ret)\n",
    "    print(\"Kurtosis: {}\".format(round(kurt+3, 6)))\n",
    "    print(\"Excess Kurtosis: {}\".format(round(kurt, 6)))\n",
    "    print(\"\")\n",
    "    \n",
    "def cVaRNP(dailyRet, per=5):\n",
    "    pci = np.percentile(dailyRet, per)\n",
    "    sumBad = 0\n",
    "    t = 0\n",
    "    for ret in dailyRet:\n",
    "        if ret <= pci:\n",
    "            t += 1\n",
    "            sumBad += ret\n",
    "    if t > 0:\n",
    "        condVaR = sumBad / t\n",
    "    else:\n",
    "        condVaR = 0\n",
    "    # print(\"CVaR at {0}%: {1}%\".format(per, np.round(float(-100*condVaR), 3)))\n",
    "    return -condVaR\n",
    "\n",
    "def riskMetricsNP(ret, name = \"portfolio\", varP = 5, cvarP = 5, port=False):\n",
    "    print(\"Risk metrics for {}\".format(name))\n",
    "    pri = indx(ret)\n",
    "    print(\"Average return: {}%\".format(round(100*np.mean(ret), 6)))\n",
    "    sd = np.std(ret)\n",
    "    # Note change for monthly to work for portfolio \n",
    "    if port: \n",
    "        print(\"SD: {}\".format(np.round(sd*np.sqrt(21), 6)))\n",
    "    else: \n",
    "        print(\"SD: {}\".format(np.round(sd, 6)))\n",
    "    semiv(ret, port=True)\n",
    "    dd = max_drawdown(pri)\n",
    "    print(\"Max DD: {}%\".format(np.round(float(100*dd), 2)))\n",
    "    valueAR(ret, varP)\n",
    "    cVaRNP(ret, cvarP)\n",
    "    sk = st.skew(ret)\n",
    "    print(\"Skewness: {}\".format(np.round(float(sk), 6)))\n",
    "    kurt = st.kurtosis(ret)\n",
    "    print(\"Kurtosis: {}\".format(round(kurt+3, 6)))\n",
    "    print(\"Excess Kurtosis: {}\".format(np.round(float(kurt), 6)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w6hoj23VhIuP"
   },
   "outputs": [],
   "source": [
    "data2000 = pd.read_csv('US2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "4-NjJSuihIuS",
    "outputId": "c67cbc80-24de-430a-9372-45b9f60612b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Chris/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/Chris/anaconda3/lib/python3.7/site-packages/pandas/core/series.py:679: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>date</th>\n",
       "      <th>PERMCO</th>\n",
       "      <th>PRC</th>\n",
       "      <th>VOL</th>\n",
       "      <th>CFACPR</th>\n",
       "      <th>sprtrn</th>\n",
       "      <th>AdjP</th>\n",
       "      <th>Ret</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.4375</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.038345</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>-0.014706</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.5625</td>\n",
       "      <td>1711.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>5.708333</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.5000</td>\n",
       "      <td>580.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>-0.007326</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.4375</td>\n",
       "      <td>1406.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.4375</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PERMNO       date  PERMCO     PRC     VOL  CFACPR    sprtrn      AdjP  \\\n",
       "0   10001 2000-01-04    7953  8.4375  1080.0     1.5 -0.038345  5.625000   \n",
       "1   10001 2000-01-05    7953  8.5625  1711.0     1.5  0.001922  5.708333   \n",
       "2   10001 2000-01-06    7953  8.5000   580.0     1.5  0.000956  5.666667   \n",
       "3   10001 2000-01-07    7953  8.4375  1406.0     1.5  0.027090  5.625000   \n",
       "4   10001 2000-01-10    7953  8.4375  3390.0     1.5  0.011190  5.625000   \n",
       "\n",
       "        Ret  year  \n",
       "0 -0.014706  2000  \n",
       "1  0.014706  2000  \n",
       "2 -0.007326  2000  \n",
       "3 -0.007380  2000  \n",
       "4  0.000000  2000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2000['date'] = pd.to_datetime(data2000['date'], format='%d/%m/%Y')\n",
    "data2000['PRC'][data2000['PRC'] < 0] *= -1\n",
    "data2000['AdjP'] = data2000['PRC'] / data2000['CFACPR']\n",
    "data2000['Ret'] = np.log(data2000['AdjP'] / data2000['AdjP'].shift())\n",
    "data2000 = data2000.groupby('PERMNO').apply(lambda x: x.iloc[1:]).reset_index(drop=True)\n",
    "data2000['year'] = data2000['date'].dt.year\n",
    "data2000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "SDm5AAW0hIuV",
    "outputId": "0578fd12-e5fd-4c04-95a0-7b82782be32c"
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    smry = pd.read_csv('smry.csv')\n",
    "    smry['year'] = pd.to_datetime(smry['year'])\n",
    "except: \n",
    "    smry = data2000.groupby(['PERMNO', 'year'], as_index=False).agg({'date': 'max', 'Ret': ['size', 'mean', 'var', st.skew, st.kurtosis, valueAR, cVaR, maxDDRet, semiv]}).dropna()\n",
    "    smry.columns = ['PERMNO', 'year', 'max', 'size', 'mean', 'variance', 'skew', 'kurtosis', 'VaR', 'CVaR', 'MaxDD', 'Semiv']\n",
    "    smry['kurtosis'] += 3\n",
    "    smry = smry[smry['size'] > 200]\n",
    "    smry[['nmean', 'nvariance', 'nskew', 'nkurtosis']] = smry[['mean', 'variance', 'skew', 'kurtosis']].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "ZagvTWRThIuY",
    "outputId": "2ff0409c-f4f1-4f90-93a1-a1ba18924d33"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>year</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>VaR</th>\n",
       "      <th>CVaR</th>\n",
       "      <th>MaxDD</th>\n",
       "      <th>Semiv</th>\n",
       "      <th>...</th>\n",
       "      <th>nskew</th>\n",
       "      <th>nkurtosis</th>\n",
       "      <th>mean2</th>\n",
       "      <th>variance2</th>\n",
       "      <th>skew2</th>\n",
       "      <th>kurtosis2</th>\n",
       "      <th>mean3</th>\n",
       "      <th>variance3</th>\n",
       "      <th>skew3</th>\n",
       "      <th>kurtosis3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>-0.013382</td>\n",
       "      <td>3.730050</td>\n",
       "      <td>0.038458</td>\n",
       "      <td>0.049853</td>\n",
       "      <td>0.152208</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.358962</td>\n",
       "      <td>11.800299</td>\n",
       "      <td>2.677344e-07</td>\n",
       "      <td>2.658095e-07</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>13.913269</td>\n",
       "      <td>1.385340e-10</td>\n",
       "      <td>1.370426e-10</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>51.897183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.358962</td>\n",
       "      <td>11.800299</td>\n",
       "      <td>0.042742</td>\n",
       "      <td>0.067054</td>\n",
       "      <td>0.305908</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258249</td>\n",
       "      <td>4.406784</td>\n",
       "      <td>4.200004e-07</td>\n",
       "      <td>8.246121e-07</td>\n",
       "      <td>0.128854</td>\n",
       "      <td>139.247057</td>\n",
       "      <td>2.721915e-10</td>\n",
       "      <td>7.488150e-10</td>\n",
       "      <td>-0.046254</td>\n",
       "      <td>1643.156918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>-0.001759</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>-0.258249</td>\n",
       "      <td>4.406784</td>\n",
       "      <td>0.033637</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.384337</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040212</td>\n",
       "      <td>9.426603</td>\n",
       "      <td>3.092481e-06</td>\n",
       "      <td>1.121894e-07</td>\n",
       "      <td>0.066693</td>\n",
       "      <td>19.419741</td>\n",
       "      <td>-5.438267e-09</td>\n",
       "      <td>3.757747e-11</td>\n",
       "      <td>-0.017223</td>\n",
       "      <td>85.578596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>-0.040212</td>\n",
       "      <td>9.426603</td>\n",
       "      <td>0.044208</td>\n",
       "      <td>0.080958</td>\n",
       "      <td>0.452394</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638468</td>\n",
       "      <td>7.658105</td>\n",
       "      <td>7.040351e-07</td>\n",
       "      <td>1.060161e-06</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>88.860835</td>\n",
       "      <td>-5.907334e-10</td>\n",
       "      <td>1.091586e-09</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>837.655778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.638468</td>\n",
       "      <td>7.658105</td>\n",
       "      <td>0.041834</td>\n",
       "      <td>0.058435</td>\n",
       "      <td>0.335233</td>\n",
       "      <td>0.018147</td>\n",
       "      <td>...</td>\n",
       "      <td>1.392394</td>\n",
       "      <td>7.646879</td>\n",
       "      <td>3.455303e-07</td>\n",
       "      <td>5.100324e-07</td>\n",
       "      <td>0.407642</td>\n",
       "      <td>58.646577</td>\n",
       "      <td>2.031090e-10</td>\n",
       "      <td>3.642475e-10</td>\n",
       "      <td>0.260266</td>\n",
       "      <td>449.121667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PERMNO       year      mean  variance      skew   kurtosis       VaR  \\\n",
       "0   10001 2000-01-01  0.000517  0.000516 -0.013382   3.730050  0.038458   \n",
       "1   10001 2001-01-01  0.000648  0.000908 -0.358962  11.800299  0.042742   \n",
       "2   10001 2002-01-01 -0.001759  0.000335 -0.258249   4.406784  0.033637   \n",
       "3   10001 2003-01-01 -0.000839  0.001030 -0.040212   9.426603  0.044208   \n",
       "4   10001 2004-01-01  0.000588  0.000714  0.638468   7.658105  0.041834   \n",
       "\n",
       "       CVaR     MaxDD     Semiv  ...     nskew  nkurtosis         mean2  \\\n",
       "0  0.049853  0.152208  0.015620  ... -0.358962  11.800299  2.677344e-07   \n",
       "1  0.067054  0.305908  0.022719  ... -0.258249   4.406784  4.200004e-07   \n",
       "2  0.043516  0.384337  0.013564  ... -0.040212   9.426603  3.092481e-06   \n",
       "3  0.080958  0.452394  0.025295  ...  0.638468   7.658105  7.040351e-07   \n",
       "4  0.058435  0.335233  0.018147  ...  1.392394   7.646879  3.455303e-07   \n",
       "\n",
       "      variance2     skew2   kurtosis2         mean3     variance3     skew3  \\\n",
       "0  2.658095e-07  0.000179   13.913269  1.385340e-10  1.370426e-10 -0.000002   \n",
       "1  8.246121e-07  0.128854  139.247057  2.721915e-10  7.488150e-10 -0.046254   \n",
       "2  1.121894e-07  0.066693   19.419741 -5.438267e-09  3.757747e-11 -0.017223   \n",
       "3  1.060161e-06  0.001617   88.860835 -5.907334e-10  1.091586e-09 -0.000065   \n",
       "4  5.100324e-07  0.407642   58.646577  2.031090e-10  3.642475e-10  0.260266   \n",
       "\n",
       "     kurtosis3  \n",
       "0    51.897183  \n",
       "1  1643.156918  \n",
       "2    85.578596  \n",
       "3   837.655778  \n",
       "4   449.121667  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smry[['nmean', 'nvariance', 'nskew', 'nkurtosis']] = smry[['mean', 'variance', 'skew', 'kurtosis']].shift(-1)\n",
    "smry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "KwN77ocdhIua",
    "outputId": "f9eb1335-c1a5-48f3-9e44-8814da11c843"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>year</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>VaR</th>\n",
       "      <th>CVaR</th>\n",
       "      <th>MaxDD</th>\n",
       "      <th>Semiv</th>\n",
       "      <th>...</th>\n",
       "      <th>nskew</th>\n",
       "      <th>nkurtosis</th>\n",
       "      <th>mean2</th>\n",
       "      <th>variance2</th>\n",
       "      <th>skew2</th>\n",
       "      <th>kurtosis2</th>\n",
       "      <th>mean3</th>\n",
       "      <th>variance3</th>\n",
       "      <th>skew3</th>\n",
       "      <th>kurtosis3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>-0.013382</td>\n",
       "      <td>3.730050</td>\n",
       "      <td>0.038458</td>\n",
       "      <td>0.049853</td>\n",
       "      <td>0.152208</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.358962</td>\n",
       "      <td>11.800299</td>\n",
       "      <td>2.677344e-07</td>\n",
       "      <td>2.658095e-07</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>13.913269</td>\n",
       "      <td>1.385340e-10</td>\n",
       "      <td>1.370426e-10</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>51.897183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.358962</td>\n",
       "      <td>11.800299</td>\n",
       "      <td>0.042742</td>\n",
       "      <td>0.067054</td>\n",
       "      <td>0.305908</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258249</td>\n",
       "      <td>4.406784</td>\n",
       "      <td>4.200004e-07</td>\n",
       "      <td>8.246121e-07</td>\n",
       "      <td>0.128854</td>\n",
       "      <td>139.247057</td>\n",
       "      <td>2.721915e-10</td>\n",
       "      <td>7.488150e-10</td>\n",
       "      <td>-0.046254</td>\n",
       "      <td>1643.156918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>-0.001759</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>-0.258249</td>\n",
       "      <td>4.406784</td>\n",
       "      <td>0.033637</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.384337</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040212</td>\n",
       "      <td>9.426603</td>\n",
       "      <td>3.092481e-06</td>\n",
       "      <td>1.121894e-07</td>\n",
       "      <td>0.066693</td>\n",
       "      <td>19.419741</td>\n",
       "      <td>-5.438267e-09</td>\n",
       "      <td>3.757747e-11</td>\n",
       "      <td>-0.017223</td>\n",
       "      <td>85.578596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>-0.040212</td>\n",
       "      <td>9.426603</td>\n",
       "      <td>0.044208</td>\n",
       "      <td>0.080958</td>\n",
       "      <td>0.452394</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638468</td>\n",
       "      <td>7.658105</td>\n",
       "      <td>7.040351e-07</td>\n",
       "      <td>1.060161e-06</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>88.860835</td>\n",
       "      <td>-5.907334e-10</td>\n",
       "      <td>1.091586e-09</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>837.655778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.638468</td>\n",
       "      <td>7.658105</td>\n",
       "      <td>0.041834</td>\n",
       "      <td>0.058435</td>\n",
       "      <td>0.335233</td>\n",
       "      <td>0.018147</td>\n",
       "      <td>...</td>\n",
       "      <td>1.392394</td>\n",
       "      <td>7.646879</td>\n",
       "      <td>3.455303e-07</td>\n",
       "      <td>5.100324e-07</td>\n",
       "      <td>0.407642</td>\n",
       "      <td>58.646577</td>\n",
       "      <td>2.031090e-10</td>\n",
       "      <td>3.642475e-10</td>\n",
       "      <td>0.260266</td>\n",
       "      <td>449.121667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   PERMNO       year      mean  variance      skew   kurtosis       VaR  \\\n",
       "0   10001 2000-01-01  0.000517  0.000516 -0.013382   3.730050  0.038458   \n",
       "1   10001 2001-01-01  0.000648  0.000908 -0.358962  11.800299  0.042742   \n",
       "2   10001 2002-01-01 -0.001759  0.000335 -0.258249   4.406784  0.033637   \n",
       "3   10001 2003-01-01 -0.000839  0.001030 -0.040212   9.426603  0.044208   \n",
       "4   10001 2004-01-01  0.000588  0.000714  0.638468   7.658105  0.041834   \n",
       "\n",
       "       CVaR     MaxDD     Semiv  ...     nskew  nkurtosis         mean2  \\\n",
       "0  0.049853  0.152208  0.015620  ... -0.358962  11.800299  2.677344e-07   \n",
       "1  0.067054  0.305908  0.022719  ... -0.258249   4.406784  4.200004e-07   \n",
       "2  0.043516  0.384337  0.013564  ... -0.040212   9.426603  3.092481e-06   \n",
       "3  0.080958  0.452394  0.025295  ...  0.638468   7.658105  7.040351e-07   \n",
       "4  0.058435  0.335233  0.018147  ...  1.392394   7.646879  3.455303e-07   \n",
       "\n",
       "      variance2     skew2   kurtosis2         mean3     variance3     skew3  \\\n",
       "0  2.658095e-07  0.000179   13.913269  1.385340e-10  1.370426e-10 -0.000002   \n",
       "1  8.246121e-07  0.128854  139.247057  2.721915e-10  7.488150e-10 -0.046254   \n",
       "2  1.121894e-07  0.066693   19.419741 -5.438267e-09  3.757747e-11 -0.017223   \n",
       "3  1.060161e-06  0.001617   88.860835 -5.907334e-10  1.091586e-09 -0.000065   \n",
       "4  5.100324e-07  0.407642   58.646577  2.031090e-10  3.642475e-10  0.260266   \n",
       "\n",
       "     kurtosis3  \n",
       "0    51.897183  \n",
       "1  1643.156918  \n",
       "2    85.578596  \n",
       "3   837.655778  \n",
       "4   449.121667  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try: \n",
    "    smry = smry.groupby('PERMNO').apply(lambda x: x.iloc[:-1]).reset_index(drop=True)\n",
    "    smry['year'] = pd.to_datetime(smry['year'], format='%Y')\n",
    "    smry = smry.drop(['max', 'size'], axis=1)\n",
    "    smry = smry.dropna()\n",
    "except: \n",
    "    pass\n",
    "smry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lFOygmdhIud"
   },
   "outputs": [],
   "source": [
    "# Add any more multiples or powers of parameters deemed necessary now \n",
    "for i in range(2,5):\n",
    "    smry['mean'+str(i)] = smry['mean'] ** i\n",
    "    smry['variance'+str(i)] = smry['variance'] ** i\n",
    "    smry['skew'+str(i)] = smry['skew'] ** i\n",
    "    smry['kurtosis'+str(i)] = smry['kurtosis'] ** i\n",
    "    \n",
    "smrycopy = copy.deepcopy(smry)\n",
    "# smry.to_csv('smry.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "RhkYoIgWhIuf",
    "outputId": "01900e6f-25c4-4158-f29d-cdc40f790b98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PERMNO', 'year', 'mean', 'variance', 'skew', 'kurtosis', 'VaR', 'CVaR',\n",
       "       'MaxDD', 'Semiv', 'nmean', 'nvariance', 'nskew', 'nkurtosis', 'mean2',\n",
       "       'variance2', 'skew2', 'kurtosis2', 'mean3', 'variance3', 'skew3',\n",
       "       'kurtosis3', 'mean4', 'variance4', 'skew4', 'kurtosis4'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smry.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "bj8qAESAhIuh",
    "outputId": "9e3a6f3a-36dd-4eb2-e686-0605273d4ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79315 entries, 0 to 79314\n",
      "Data columns (total 26 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   PERMNO     79315 non-null  int64         \n",
      " 1   year       79315 non-null  datetime64[ns]\n",
      " 2   mean       79315 non-null  float64       \n",
      " 3   variance   79315 non-null  float64       \n",
      " 4   skew       79315 non-null  float64       \n",
      " 5   kurtosis   79315 non-null  float64       \n",
      " 6   VaR        79315 non-null  float64       \n",
      " 7   CVaR       79315 non-null  float64       \n",
      " 8   MaxDD      79315 non-null  float64       \n",
      " 9   Semiv      79315 non-null  float64       \n",
      " 10  nmean      79315 non-null  float64       \n",
      " 11  nvariance  79315 non-null  float64       \n",
      " 12  nskew      79315 non-null  float64       \n",
      " 13  nkurtosis  79315 non-null  float64       \n",
      " 14  mean2      79315 non-null  float64       \n",
      " 15  variance2  79315 non-null  float64       \n",
      " 16  skew2      79315 non-null  float64       \n",
      " 17  kurtosis2  79315 non-null  float64       \n",
      " 18  mean3      79315 non-null  float64       \n",
      " 19  variance3  79315 non-null  float64       \n",
      " 20  skew3      79315 non-null  float64       \n",
      " 21  kurtosis3  79315 non-null  float64       \n",
      " 22  mean4      79315 non-null  float64       \n",
      " 23  variance4  79315 non-null  float64       \n",
      " 24  skew4      79315 non-null  float64       \n",
      " 25  kurtosis4  79315 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(24), int64(1)\n",
      "memory usage: 15.7 MB\n"
     ]
    }
   ],
   "source": [
    "smry.describe()\n",
    "smry.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUFGOc-DhIui"
   },
   "outputs": [],
   "source": [
    "# alterations to smry to test adjustments \n",
    "sqsmry = copy.deepcopy(smry)\n",
    "sqsmry.iloc[:, 2:] = sqsmry[:].iloc[:,2:] ** 2\n",
    "logsqsmry = copy.copy(sqsmry)\n",
    "logsqsmry.iloc[:, 2:] = np.log(logsqsmry[:].iloc[:, 2:] + 1)\n",
    "absmry = copy.deepcopy(smry) \n",
    "absmry.iloc[:, 2:] = abs(absmry.iloc[:, 2:])\n",
    "logabsmry = copy.copy(absmry) \n",
    "logabsmry.iloc[:, 2:] = np.log(absmry.iloc[:, 2:] + 1)\n",
    "plusmry = copy.deepcopy(smry)\n",
    "plusmry.iloc[:, 2:] = plusmry.iloc[:, 2:] - np.min(plusmry.iloc[:, 2:])\n",
    "logplusmry = copy.copy(plusmry) \n",
    "logplusmry.iloc[:, 2:] = np.log(plusmry.iloc[:, 2:] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "colab_type": "code",
    "id": "9FiLsa1ohIuk",
    "outputId": "f6cb4b8e-5968-40ee-9f4f-394a59b10798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0 79315     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>year</th>\n",
       "      <th>mean</th>\n",
       "      <th>variance</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurtosis</th>\n",
       "      <th>VaR</th>\n",
       "      <th>CVaR</th>\n",
       "      <th>MaxDD</th>\n",
       "      <th>Semiv</th>\n",
       "      <th>...</th>\n",
       "      <th>skew2</th>\n",
       "      <th>kurtosis2</th>\n",
       "      <th>mean3</th>\n",
       "      <th>variance3</th>\n",
       "      <th>skew3</th>\n",
       "      <th>kurtosis3</th>\n",
       "      <th>mean4</th>\n",
       "      <th>variance4</th>\n",
       "      <th>skew4</th>\n",
       "      <th>kurtosis4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>-0.013382</td>\n",
       "      <td>3.730050</td>\n",
       "      <td>0.038458</td>\n",
       "      <td>0.049853</td>\n",
       "      <td>0.152208</td>\n",
       "      <td>0.015620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000179</td>\n",
       "      <td>13.913269</td>\n",
       "      <td>1.385340e-10</td>\n",
       "      <td>1.370426e-10</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>51.897183</td>\n",
       "      <td>7.168172e-14</td>\n",
       "      <td>7.065466e-14</td>\n",
       "      <td>3.207162e-08</td>\n",
       "      <td>193.579063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.358962</td>\n",
       "      <td>11.800299</td>\n",
       "      <td>0.042742</td>\n",
       "      <td>0.067054</td>\n",
       "      <td>0.305908</td>\n",
       "      <td>0.022719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128854</td>\n",
       "      <td>139.247057</td>\n",
       "      <td>2.721915e-10</td>\n",
       "      <td>7.488150e-10</td>\n",
       "      <td>-0.046254</td>\n",
       "      <td>1643.156918</td>\n",
       "      <td>1.764003e-13</td>\n",
       "      <td>6.799851e-13</td>\n",
       "      <td>1.660329e-02</td>\n",
       "      <td>19389.743001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>2002-01-01</td>\n",
       "      <td>-0.001759</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>-0.258249</td>\n",
       "      <td>4.406784</td>\n",
       "      <td>0.033637</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.384337</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066693</td>\n",
       "      <td>19.419741</td>\n",
       "      <td>-5.438267e-09</td>\n",
       "      <td>3.757747e-11</td>\n",
       "      <td>-0.017223</td>\n",
       "      <td>85.578596</td>\n",
       "      <td>9.563438e-12</td>\n",
       "      <td>1.258646e-14</td>\n",
       "      <td>4.447905e-03</td>\n",
       "      <td>377.126349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>2003-01-01</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>-0.040212</td>\n",
       "      <td>9.426603</td>\n",
       "      <td>0.044208</td>\n",
       "      <td>0.080958</td>\n",
       "      <td>0.452394</td>\n",
       "      <td>0.025295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>88.860835</td>\n",
       "      <td>-5.907334e-10</td>\n",
       "      <td>1.091586e-09</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>837.655778</td>\n",
       "      <td>4.956655e-13</td>\n",
       "      <td>1.123942e-12</td>\n",
       "      <td>2.614628e-06</td>\n",
       "      <td>7896.248085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.638468</td>\n",
       "      <td>7.658105</td>\n",
       "      <td>0.041834</td>\n",
       "      <td>0.058435</td>\n",
       "      <td>0.335233</td>\n",
       "      <td>0.018147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.407642</td>\n",
       "      <td>58.646577</td>\n",
       "      <td>2.031090e-10</td>\n",
       "      <td>3.642475e-10</td>\n",
       "      <td>0.260266</td>\n",
       "      <td>449.121667</td>\n",
       "      <td>1.193912e-13</td>\n",
       "      <td>2.601330e-13</td>\n",
       "      <td>1.661719e-01</td>\n",
       "      <td>3439.421038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79310</th>\n",
       "      <td>93436</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.529470</td>\n",
       "      <td>5.987900</td>\n",
       "      <td>0.049580</td>\n",
       "      <td>0.065687</td>\n",
       "      <td>0.294424</td>\n",
       "      <td>0.019815</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280338</td>\n",
       "      <td>35.854952</td>\n",
       "      <td>2.140477e-11</td>\n",
       "      <td>1.040549e-09</td>\n",
       "      <td>0.148431</td>\n",
       "      <td>214.695885</td>\n",
       "      <td>5.943113e-15</td>\n",
       "      <td>1.054428e-12</td>\n",
       "      <td>7.858950e-02</td>\n",
       "      <td>1285.577590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79311</th>\n",
       "      <td>93436</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>-0.682840</td>\n",
       "      <td>10.754609</td>\n",
       "      <td>0.046433</td>\n",
       "      <td>0.077833</td>\n",
       "      <td>0.348228</td>\n",
       "      <td>0.025978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466270</td>\n",
       "      <td>115.661607</td>\n",
       "      <td>3.173403e-10</td>\n",
       "      <td>1.492081e-09</td>\n",
       "      <td>-0.318388</td>\n",
       "      <td>1243.895313</td>\n",
       "      <td>2.164547e-13</td>\n",
       "      <td>1.704996e-12</td>\n",
       "      <td>2.174079e-01</td>\n",
       "      <td>13377.607268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79312</th>\n",
       "      <td>93436</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>0.005917</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.566829</td>\n",
       "      <td>7.976268</td>\n",
       "      <td>0.055171</td>\n",
       "      <td>0.084741</td>\n",
       "      <td>0.786385</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321295</td>\n",
       "      <td>63.620850</td>\n",
       "      <td>2.071084e-07</td>\n",
       "      <td>5.368356e-09</td>\n",
       "      <td>0.182119</td>\n",
       "      <td>507.456949</td>\n",
       "      <td>1.225360e-09</td>\n",
       "      <td>9.399867e-12</td>\n",
       "      <td>1.032302e-01</td>\n",
       "      <td>4047.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79313</th>\n",
       "      <td>93436</td>\n",
       "      <td>2014-01-01</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.486219</td>\n",
       "      <td>7.087730</td>\n",
       "      <td>0.043235</td>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.430912</td>\n",
       "      <td>0.019443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.236409</td>\n",
       "      <td>50.235916</td>\n",
       "      <td>3.736225e-09</td>\n",
       "      <td>7.464386e-10</td>\n",
       "      <td>0.114947</td>\n",
       "      <td>356.058611</td>\n",
       "      <td>5.797543e-12</td>\n",
       "      <td>6.771093e-13</td>\n",
       "      <td>5.588928e-02</td>\n",
       "      <td>2523.647290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79314</th>\n",
       "      <td>93436</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.061185</td>\n",
       "      <td>5.050386</td>\n",
       "      <td>0.043085</td>\n",
       "      <td>0.054574</td>\n",
       "      <td>0.293539</td>\n",
       "      <td>0.016822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>25.506400</td>\n",
       "      <td>2.760230e-11</td>\n",
       "      <td>2.123158e-10</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>128.817170</td>\n",
       "      <td>8.341813e-15</td>\n",
       "      <td>1.266611e-13</td>\n",
       "      <td>1.401449e-05</td>\n",
       "      <td>650.576450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79315 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PERMNO       year      mean  variance      skew   kurtosis       VaR  \\\n",
       "0       10001 2000-01-01  0.000517  0.000516 -0.013382   3.730050  0.038458   \n",
       "1       10001 2001-01-01  0.000648  0.000908 -0.358962  11.800299  0.042742   \n",
       "2       10001 2002-01-01 -0.001759  0.000335 -0.258249   4.406784  0.033637   \n",
       "3       10001 2003-01-01 -0.000839  0.001030 -0.040212   9.426603  0.044208   \n",
       "4       10001 2004-01-01  0.000588  0.000714  0.638468   7.658105  0.041834   \n",
       "...       ...        ...       ...       ...       ...        ...       ...   \n",
       "79310   93436 2011-01-01  0.000278  0.001013  0.529470   5.987900  0.049580   \n",
       "79311   93436 2012-01-01  0.000682  0.001143 -0.682840  10.754609  0.046433   \n",
       "79312   93436 2013-01-01  0.005917  0.001751  0.566829   7.976268  0.055171   \n",
       "79313   93436 2014-01-01  0.001552  0.000907  0.486219   7.087730  0.043235   \n",
       "79314   93436 2015-01-01  0.000302  0.000597  0.061185   5.050386  0.043085   \n",
       "\n",
       "           CVaR     MaxDD     Semiv  ...     skew2   kurtosis2         mean3  \\\n",
       "0      0.049853  0.152208  0.015620  ...  0.000179   13.913269  1.385340e-10   \n",
       "1      0.067054  0.305908  0.022719  ...  0.128854  139.247057  2.721915e-10   \n",
       "2      0.043516  0.384337  0.013564  ...  0.066693   19.419741 -5.438267e-09   \n",
       "3      0.080958  0.452394  0.025295  ...  0.001617   88.860835 -5.907334e-10   \n",
       "4      0.058435  0.335233  0.018147  ...  0.407642   58.646577  2.031090e-10   \n",
       "...         ...       ...       ...  ...       ...         ...           ...   \n",
       "79310  0.065687  0.294424  0.019815  ...  0.280338   35.854952  2.140477e-11   \n",
       "79311  0.077833  0.348228  0.025978  ...  0.466270  115.661607  3.173403e-10   \n",
       "79312  0.084741  0.786385  0.028234  ...  0.321295   63.620850  2.071084e-07   \n",
       "79313  0.062884  0.430912  0.019443  ...  0.236409   50.235916  3.736225e-09   \n",
       "79314  0.054574  0.293539  0.016822  ...  0.003744   25.506400  2.760230e-11   \n",
       "\n",
       "          variance3     skew3    kurtosis3         mean4     variance4  \\\n",
       "0      1.370426e-10 -0.000002    51.897183  7.168172e-14  7.065466e-14   \n",
       "1      7.488150e-10 -0.046254  1643.156918  1.764003e-13  6.799851e-13   \n",
       "2      3.757747e-11 -0.017223    85.578596  9.563438e-12  1.258646e-14   \n",
       "3      1.091586e-09 -0.000065   837.655778  4.956655e-13  1.123942e-12   \n",
       "4      3.642475e-10  0.260266   449.121667  1.193912e-13  2.601330e-13   \n",
       "...             ...       ...          ...           ...           ...   \n",
       "79310  1.040549e-09  0.148431   214.695885  5.943113e-15  1.054428e-12   \n",
       "79311  1.492081e-09 -0.318388  1243.895313  2.164547e-13  1.704996e-12   \n",
       "79312  5.368356e-09  0.182119   507.456949  1.225360e-09  9.399867e-12   \n",
       "79313  7.464386e-10  0.114947   356.058611  5.797543e-12  6.771093e-13   \n",
       "79314  2.123158e-10  0.000229   128.817170  8.341813e-15  1.266611e-13   \n",
       "\n",
       "              skew4     kurtosis4  \n",
       "0      3.207162e-08    193.579063  \n",
       "1      1.660329e-02  19389.743001  \n",
       "2      4.447905e-03    377.126349  \n",
       "3      2.614628e-06   7896.248085  \n",
       "4      1.661719e-01   3439.421038  \n",
       "...             ...           ...  \n",
       "79310  7.858950e-02   1285.577590  \n",
       "79311  2.174079e-01  13377.607268  \n",
       "79312  1.032302e-01   4047.612600  \n",
       "79313  5.588928e-02   2523.647290  \n",
       "79314  1.401449e-05    650.576450  \n",
       "\n",
       "[79315 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum((smrycopy - smry).values != 0))\n",
    "smrycopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "smry = pd.read_csv('smry.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckDmWx8LhIum"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UZTvrZYMhIuo"
   },
   "outputs": [],
   "source": [
    "X = smry.drop(['PERMNO', 'year', 'nmean', 'nvariance', 'nskew', 'nkurtosis'],axis=1).values\n",
    "y = smry[['nmean', 'nvariance', 'nskew', 'nkurtosis']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SF_F0mfhIup"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3F5nuQ1hIur"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# X_train = scaler.fit_transform(np.log(scaler.fit_transform(X_train) + 1))\n",
    "X_test = scaler.transform(X_test)\n",
    "# X_test = scaler.fit_transform(np.log(scaler.fit_transform(X_test) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4-wyIn8hIus"
   },
   "outputs": [],
   "source": [
    "# 2 hidden layers \n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(20,activation='relu'))\n",
    "\n",
    "for i in range(3):\n",
    "    model.add(Dense(20,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Dense(4))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Cb5bN4bhIut"
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SxjLQfKghIuv",
    "outputId": "d409c1e4-4040-4553-e9cc-f4ff19e9be2f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 45.7054 - val_loss: 37.6752\n",
      "Epoch 2/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 41.4345 - val_loss: 37.7765\n",
      "Epoch 3/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 41.0848 - val_loss: 37.7127\n",
      "Epoch 4/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.9260 - val_loss: 37.5374\n",
      "Epoch 5/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.7164 - val_loss: 37.4979\n",
      "Epoch 6/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.6331 - val_loss: 37.3036\n",
      "Epoch 7/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.4631 - val_loss: 37.3661\n",
      "Epoch 8/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.4046 - val_loss: 37.2064\n",
      "Epoch 9/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.3888 - val_loss: 37.2096\n",
      "Epoch 10/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.2988 - val_loss: 37.1543\n",
      "Epoch 11/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.2698 - val_loss: 37.0509\n",
      "Epoch 12/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.2889 - val_loss: 37.0285\n",
      "Epoch 13/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.2906 - val_loss: 37.0593\n",
      "Epoch 14/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.1546 - val_loss: 37.1339\n",
      "Epoch 15/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.1325 - val_loss: 37.0175\n",
      "Epoch 16/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0577 - val_loss: 37.0646\n",
      "Epoch 17/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0866 - val_loss: 37.0034\n",
      "Epoch 18/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.2014 - val_loss: 37.0776\n",
      "Epoch 19/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0591 - val_loss: 37.0204\n",
      "Epoch 20/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0568 - val_loss: 36.9763\n",
      "Epoch 21/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0336 - val_loss: 37.0267\n",
      "Epoch 22/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0413 - val_loss: 36.9547\n",
      "Epoch 23/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.9766 - val_loss: 37.0167\n",
      "Epoch 24/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0469 - val_loss: 37.0694\n",
      "Epoch 25/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8897 - val_loss: 36.9196\n",
      "Epoch 26/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0029 - val_loss: 36.9943\n",
      "Epoch 27/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.9576 - val_loss: 36.9554\n",
      "Epoch 28/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0210 - val_loss: 36.9321\n",
      "Epoch 29/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.9022 - val_loss: 36.9162\n",
      "Epoch 30/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 40.0090 - val_loss: 37.0171\n",
      "Epoch 31/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.9496 - val_loss: 36.9455\n",
      "Epoch 32/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8915 - val_loss: 37.0347\n",
      "Epoch 33/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8916 - val_loss: 36.9264\n",
      "Epoch 34/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8955 - val_loss: 36.8920\n",
      "Epoch 35/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.9437 - val_loss: 37.0489\n",
      "Epoch 36/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8494 - val_loss: 36.9290\n",
      "Epoch 37/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8639 - val_loss: 36.8885\n",
      "Epoch 38/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.9353 - val_loss: 37.0032\n",
      "Epoch 39/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8858 - val_loss: 36.9337\n",
      "Epoch 40/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8613 - val_loss: 36.9189\n",
      "Epoch 41/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7886 - val_loss: 36.9176\n",
      "Epoch 42/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8376 - val_loss: 36.9706\n",
      "Epoch 43/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8297 - val_loss: 37.0092\n",
      "Epoch 44/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8119 - val_loss: 36.8804\n",
      "Epoch 45/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8060 - val_loss: 36.9652\n",
      "Epoch 46/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8349 - val_loss: 36.8912\n",
      "Epoch 47/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7612 - val_loss: 36.9172\n",
      "Epoch 48/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8134 - val_loss: 36.8918\n",
      "Epoch 49/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7718 - val_loss: 36.9448\n",
      "Epoch 50/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7836 - val_loss: 36.8727\n",
      "Epoch 51/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8318 - val_loss: 36.9670\n",
      "Epoch 52/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7603 - val_loss: 36.9196\n",
      "Epoch 53/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.8023 - val_loss: 36.9802\n",
      "Epoch 54/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7464 - val_loss: 36.9171\n",
      "Epoch 55/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7444 - val_loss: 37.0333\n",
      "Epoch 56/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7304 - val_loss: 36.8983\n",
      "Epoch 57/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7128 - val_loss: 36.9981\n",
      "Epoch 58/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7587 - val_loss: 36.9151\n",
      "Epoch 59/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7822 - val_loss: 36.9244\n",
      "Epoch 60/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7263 - val_loss: 36.8793\n",
      "Epoch 61/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7168 - val_loss: 36.9339\n",
      "Epoch 62/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6328 - val_loss: 36.8438\n",
      "Epoch 63/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7106 - val_loss: 36.8968\n",
      "Epoch 64/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7168 - val_loss: 36.9038\n",
      "Epoch 65/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7548 - val_loss: 36.8907\n",
      "Epoch 66/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7535 - val_loss: 36.9153\n",
      "Epoch 67/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6619 - val_loss: 36.9204\n",
      "Epoch 68/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6415 - val_loss: 36.8740\n",
      "Epoch 69/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6884 - val_loss: 36.8963\n",
      "Epoch 70/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7119 - val_loss: 36.9336\n",
      "Epoch 71/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6629 - val_loss: 36.8646\n",
      "Epoch 72/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.7652 - val_loss: 36.9360\n",
      "Epoch 73/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6872 - val_loss: 36.9101\n",
      "Epoch 74/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6651 - val_loss: 36.9392\n",
      "Epoch 75/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6383 - val_loss: 36.9091\n",
      "Epoch 76/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6186 - val_loss: 36.9471\n",
      "Epoch 77/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6621 - val_loss: 36.9854\n",
      "Epoch 78/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6376 - val_loss: 36.8729\n",
      "Epoch 79/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6506 - val_loss: 36.9345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6467 - val_loss: 36.9382\n",
      "Epoch 81/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6036 - val_loss: 36.9375\n",
      "Epoch 82/150\n",
      "283/283 [==============================] - 0s 1ms/step - loss: 39.6762 - val_loss: 36.8879\n",
      "Epoch 00082: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff6577b6750>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, \n",
    "         y=y_train, \n",
    "         epochs=150, \n",
    "          batch_size=256, \n",
    "          validation_data=(X_test, y_test), \n",
    "          callbacks=[early_stop]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "alGy110VhIuw",
    "outputId": "d63c1ab6-b80f-478c-c241-3ed813f7dc61"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "no numeric data to plot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-17f36f0e4005>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/plotting/_core.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mplot_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0m__call__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/plotting/_matplotlib/__init__.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(data, kind, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ax\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left_ax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mplot_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLOT_CLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mplot_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_adjust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_plot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_subplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/plotting/_matplotlib/core.py\u001b[0m in \u001b[0;36m_compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# no non-numeric frames or series allowed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no numeric data to plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0;31m# GH25587: cast ExtensionArray of pandas (IntegerArray, etc.) to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no numeric data to plot"
     ]
    }
   ],
   "source": [
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "l_4Tms4rhIux",
    "outputId": "467f083b-9d71-406c-bf6b-de516c027e36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.5014937e-04,  1.0616789e-03, -1.3411489e-01,  8.3517666e+00],\n",
       "       [ 8.9446385e-04,  1.3147183e-03, -1.1500132e-01,  9.4057875e+00],\n",
       "       [ 8.1997208e-04,  1.1788096e-03, -1.2490746e-01,  8.8547659e+00],\n",
       "       ...,\n",
       "       [ 9.0669160e-04,  1.3429506e-03, -1.1328645e-01,  9.5048189e+00],\n",
       "       [ 7.9773826e-04,  1.1394683e-03, -1.2784767e-01,  8.6921597e+00],\n",
       "       [ 8.5341174e-04,  1.2384235e-03, -1.2048438e-01,  9.1000223e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "0uUqkzr9hIuz",
    "outputId": "5fa9b188-8be1-40f0-82a9-273213b880b3",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff6434a5f90>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test[:,0], pred[:,0])\n",
    "plt.scatter(y_test[:,0], y_test[:,0], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "eaBV08INhIu0",
    "outputId": "5856a877-7a17-4c48-eb46-9ae6704b630b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff646ee6d10>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAekAAAFlCAYAAADPvBA0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAf80lEQVR4nO3df4yd1X3n8ffX4x94aBenxpUWgz3OQto1SZu0U9DuNu1uHBKybeJ0S4NTN2UVVm7CInWVbRpnvdsStN6WRm22UkhTS1AhuFvIogbNNqksNVStGqXU4wAlJuvVQDDYdLXGGCoYJ7bhu3/cZ+B6uON57sz9cebO+yWN5j7nOXfme2Y88/E5z7nPRGYiSZLKs2LQBUiSpPYMaUmSCmVIS5JUKENakqRCGdKSJBXKkJYkqVArB13AbBdffHGOjY0NugxJkvrm4MGDz2XmhtntxYX02NgYk5OTgy5DkqS+iYgj7dpd7pYkqVCGtCRJhTKkJUkqlCEtSVKhDGlJkgplSEuSVChDWpKkQhnSkiQVypCWJKlQhrQkSXU0GjA2BitWNN83Gj3/lMXdFlSSpOI0GrBrF0xPN4+PHGkeA+zc2bNP60xakqT57NnzekDPmJ5utveQIS1J0nyefrqz9i4xpCVJms+mTZ21d4khLUnSfPbuhdHRc9tGR5vtPWRIS5I0n507Yd8+2LwZIprv9+3r6aYxcHe3JEn17NzZ81CezZm0JEmFMqQlSSqUIS1JUqFqhXREXBsRhyNiKiJ2tzm/JiLuq84/FBFjVfvOiHik5e3ViHh7d4cgSdJwmjekI2IEuB14H7AV+HBEbJ3V7UbgZGZeDnwOuA0gMxuZ+fbMfDvwEeA7mflINwcgSdKwqjOTvgqYyswnM/M0cC+wfVaf7cBd1eP7gW0REbP6fLh6riRJqqFOSG8Enmk5Plq1te2TmWeBF4H1s/pcD/zxwsqUJGn56cvGsYi4GpjOzG/NcX5XRExGxOTx48f7UZIkScWrE9LHgMtaji+t2tr2iYiVwEXAiZbzOzjPLDoz92XmeGaOb9iwoU7dkiQNvTohfQC4IiK2RMRqmoE7MavPBHBD9fg64MHMTICIWAF8CK9HS5LUkXlvC5qZZyPiZmA/MALcmZmHIuJWYDIzJ4A7gLsjYgp4nmaQz/gp4JnMfLL75UuSNLyimvAWY3x8PCcnJwddhiRJfRMRBzNzfHa7dxyTJKlQhrQkSYUypCVJKpQhLUlSoQxpSZIKZUhLklQoQ1qSpEIZ0pIkFcqQliSpUIa0JEmFMqQlSSqUIS1JUqEMaUmSCmVIS5JUKENakqRCGdKSJBXKkJYkqVCGtCRJhTKkJUkqlCEtSVKhDGlJkgplSEuSVChDWpKkQhnSkiQVypCWJKlQhrQkSYUypCVJKpQhLUlSoQxpSZIKZUhLklQoQ1qSpEIZ0pIkFapWSEfEtRFxOCKmImJ3m/NrIuK+6vxDETHWcu5HIuIbEXEoIh6LiAu6V74kqesaDRgbgxUrmu8bjUFXtGytnK9DRIwAtwPXAEeBAxExkZmPt3S7ETiZmZdHxA7gNuD6iFgJ3AN8JDMfjYj1wJmuj0KS1B2NBuzaBdPTzeMjR5rHADt3Dq6uZarOTPoqYCozn8zM08C9wPZZfbYDd1WP7we2RUQA7wH+LjMfBcjME5n5SndKlyR13Z49rwf0jOnpZrv6rk5IbwSeaTk+WrW17ZOZZ4EXgfXAW4CMiP0R8c2I+PXFlyxJ6pmnn+6sXT3V641jK4GfBHZW738uIrbN7hQRuyJiMiImjx8/3uOSJElz2rSps3b1VJ2QPgZc1nJ8adXWtk91Hfoi4ATNWfdfZeZzmTkNfBX4sdmfIDP3ZeZ4Zo5v2LCh81FIkrpj714YHT23bXS02a6+qxPSB4ArImJLRKwGdgATs/pMADdUj68DHszMBPYDb4uI0Sq8fxp4HElSmXbuhH37YPNmiGi+37fPTWMDMu/u7sw8GxE30wzcEeDOzDwUEbcCk5k5AdwB3B0RU8DzNIOczDwZEb9HM+gT+GpmfqVHY5EkdcPOnYZyIaI54S3H+Ph4Tk5ODroMSZL6JiIOZub47HbvOCZJUqEMaUmSCmVIS5JUKENakqRCGdKSJBXKkJYkqVCGtCRJhTKkJUkqlCEtSVKhDGlJkgplSEuSVChDWpKkQhnSkiQVypCWJKlQhrQkSYUypCVJKpQhLUlSoQxpSZIKZUhLklQoQ1qSpEIZ0pIkFcqQliSpUIa0JEmFMqQlSSqUIS1JUqEMaUmSCmVIS5JUKENakqRCGdKSJBXKkJYkqVCGtCRJhTKkJUkqVK2QjohrI+JwRExFxO4259dExH3V+YciYqxqH4uIUxHxSPX2xe6WL0nS8Fo5X4eIGAFuB64BjgIHImIiMx9v6XYjcDIzL4+IHcBtwPXVuScy8+1drluSpKFXZyZ9FTCVmU9m5mngXmD7rD7bgbuqx/cD2yIiulemJEnLT52Q3gg803J8tGpr2yczzwIvAuurc1si4uGI+MuIeGe7TxARuyJiMiImjx8/3tEAJEkaVr3eOPb3wKbMfAfwCeB/RMQ/mt0pM/dl5nhmjm/YsKHHJUmStDTUCeljwGUtx5dWbW37RMRK4CLgRGZ+LzNPAGTmQeAJ4C2LLVqSpOWgTkgfAK6IiC0RsRrYAUzM6jMB3FA9vg54MDMzIjZUG8+IiDcDVwBPdqd0SZKG27y7uzPzbETcDOwHRoA7M/NQRNwKTGbmBHAHcHdETAHP0wxygJ8Cbo2IM8CrwMcy8/leDESSpGETmTnoGs4xPj6ek5OTgy5DkqS+iYiDmTk+u907jkmSVChDWpKkQhnSkiQVypCWJKlQhrQkSYUypCVJKpQhLUlSoQxpSZIKZUhLklQoQ1qSpEIZ0pIkFcqQliSpUIa0JEmFMqQlSSqUIS1JUqEMaUmSCmVIS5JUKENakqRCGdKSJBXKkJYkqVCGtCRJhTKkJUkqlCEtSVKhDGlJkgplSEuSVChDWpKkQhnSkiQVypCWJKlQhrQkSYUypCVJKpQhLUlSoQxpSZIKVSukI+LaiDgcEVMRsbvN+TURcV91/qGIGJt1flNEvBQRv9adsiVJGn7zhnREjAC3A+8DtgIfjoits7rdCJzMzMuBzwG3zTr/e8CfLb5cSZKWjzoz6auAqcx8MjNPA/cC22f12Q7cVT2+H9gWEQEQER8EvgMc6k7JkiQtD3VCeiPwTMvx0aqtbZ/MPAu8CKyPiO8DPgV85nyfICJ2RcRkREweP368bu2SJA21Xm8cuwX4XGa+dL5OmbkvM8czc3zDhg09LkmSpKVhZY0+x4DLWo4vrdra9TkaESuBi4ATwNXAdRHxO8A64NWI+G5mfn7RlUuSNOTqhPQB4IqI2EIzjHcAvzirzwRwA/AN4DrgwcxM4J0zHSLiFuAlA1qSpHrmDenMPBsRNwP7gRHgzsw8FBG3ApOZOQHcAdwdEVPA8zSDXJIkLUI0J7zlGB8fz8nJyUGXIUlS30TEwcwcn93uHcckSSqUIS1JUqEMaUmSCmVIS5JUKENakqRCGdKSmhoNGBuDFSua7xuNQVckLXt1bmYiadg1GrBrF0xPN4+PHGkeA+zcObi6pGXOmbQk2LPn9YCeMT3dbJc0MIa0JHj66c7aJfWFIS0JNm3qrF1SXxjSkmDvXhgdPbdtdLTZLmlgDGlJzc1h+/bB5s0Q0Xy/b5+bxqQBc3e3pKadOw1lqTDOpCVJKpQhLUlSoQxpSZIKZUhLklQoQ1qSpEIZ0pIkFcqQliSpUIa0JEmFMqQlSSqUIS1JUqEMaUmSCmVIS5JUKENakqRCGdKSJBXKkJYkqVCGtCRJhTKkpRI0GjA2BitWNN83GoOuSFIBVg66AGnZazRg1y6Ynm4eHznSPAbYuXNwdUkauFoz6Yi4NiIOR8RUROxuc35NRNxXnX8oIsaq9qsi4pHq7dGI+Lnuli8NgT17Xg/oGdPTzXZJy9q8IR0RI8DtwPuArcCHI2LrrG43Aicz83Lgc8BtVfu3gPHMfDtwLfCHEeHsXWr19NOdtUtaNurMpK8CpjLzycw8DdwLbJ/VZztwV/X4fmBbRERmTmfm2ar9AiC7UbS0pM2+/vwDP9C+36ZN/axKUoHqhPRG4JmW46NVW9s+VSi/CKwHiIirI+IQ8BjwsZbQlpafmevPR45AZvP9P/wDrF59br/RUdi7dzA1SipGz3d3Z+ZDmXkl8BPApyPigtl9ImJXRExGxOTx48d7XZI0OO2uP585A9///bB5M0Q03+/b56YxSbV2dx8DLms5vrRqa9fnaHXN+SLgRGuHzPx2RLwEvBWYnHVuH7APYHx83CVxDa+5rjM//zw891x/a5FUvDoz6QPAFRGxJSJWAzuAiVl9JoAbqsfXAQ9mZlbPWQkQEZuBHwae6krl0lI013Vmrz9LamPekK6uId8M7Ae+DXwpMw9FxK0R8YGq2x3A+oiYAj4BzLxM6yeBRyPiEeDLwE2Z6XRBy9fevc3rza28/ixpDpFZ1ury+Ph4Tk5Ozt9RWqoajea16aefbs6g9+71+rO0zEXEwcwcn93ua5alftu501CWVIv37pYWq9GAiy9u7syOaD723tuSusCZtLRQjQb86q/CiRPntp84AR/9aPOxM2ZJi+BMWlqImZuSzA7oGadPe+9tSYvmTFrqVKMBN9wAr7xy/n7ee1vSIjmTljoxM4OeL6DB1z5LWjRDWupEu9t6trN6ta99lrRohrTUiTpL2OvXw513umlM0qIZ0lIn5lrCHhmBe+5p/mWr554zoCV1hSEtdWKu23redZfBLKnrDGmpEzt3Nv+MpH9WUlIf+BIsqVPe1lNSnziTliSpUIa0JEmFMqQlSSqUIS1JUqEMaUmSCmVIS5JUKENakqRCGdKSJBXKkJYkqVCGtCRJhTKkJUkqlCEtSVKhDGlJkgplSGvpazRgbAxWrGi+bzQGXZEkdYV/qlJLV6MBH/sYvPTS621HjsCuXc3H/jlJSUucM2ktTY0GfPSj5wb0jOlp2LOn/zVJUpcZ0lqa9uyB06fnPv/00/2rRZJ6xJDW0tF67fnIkfP33bSpLyVJUi95TVrlazTgV34FXn65Xv8I2Lu3tzVJUh84k1bZbroJfumX6gc0NDeTuWlM0hCoFdIRcW1EHI6IqYjY3eb8moi4rzr/UESMVe3XRMTBiHisev+u7pavodZowBe/WL//+vVwzz3whS/0riZJ6qN5l7sjYgS4HbgGOAociIiJzHy8pduNwMnMvDwidgC3AdcDzwHvz8xnI+KtwH5gY7cHoSHTaDQ3hs133Rlg82Z46qmelyRJg1BnJn0VMJWZT2bmaeBeYPusPtuBu6rH9wPbIiIy8+HMfLZqPwSsjYg13ShcQ6rRaL7OuU5Ae+1Z0pCrE9IbgWdajo/yxtnwa30y8yzwIrB+Vp+fB76Zmd+b/QkiYldETEbE5PHjx+vWrmHSaMDFFzevP09P13uO154lDbm+bByLiCtpLoH/SrvzmbkvM8czc3zDhg39KEklmdkcduJEvf4XXui1Z0nLQp2QPgZc1nJ8adXWtk9ErAQuAk5Ux5cCXwZ+OTOfWGzBGjKdbA7bvLkZzi+95Axa0rJQJ6QPAFdExJaIWA3sACZm9ZkAbqgeXwc8mJkZEeuArwC7M/Pr3SpaQ2TPHsg8f5/R0WY4P/WU4SxpWZk3pKtrzDfT3Jn9beBLmXkoIm6NiA9U3e4A1kfEFPAJYOZlWjcDlwO/ERGPVG8/2PVRaGnp5M5hIyOwb5/hLGlZipxvFtNn4+PjOTk5Oegy1Cszu7frbA5btQr+6I8MaElDLyIOZub47HbvOKb+2rOnXkBfeKEBLWnZM6TVe3WXtyPcHCZJLfwDG+qtusvb3jlMkt7AmbR6Y2b2XOfmJKOj3jlMktowpNVdrXcOm2/n9szytru3Jaktl7vVPY1GM5zrcHlbkublTFrd0UlAu7wtSbUY0lqc1uXtOlzelqTaXO7Wwt10U/O+23VviLN+vUvcktQBQ1oLc9NN8Ad/UL//6tXw+7/fu3okaQgZ0urcu98NX/ta/f4jI3DnnS5xS1KHDGnV12k4A2zdCocO9aYeSRpybhxTPRs31g/o9eubt/bMNKAlaREMaZ1fowFr1sCzz9br//GPw3PPubQtSV1gSGtuN93UfGnV6dP1+n/84/CFL/S2JklaRgxptdfp7u1t2wxoSeoyN47pXAvZHLZtG/z5n/emHklaxgxpve5Nb4IXXqjfPwLuvtvrz5LUI4a0mjZu7CygnT1LUs95TXq5u+mm5oy47u5taG4QM6AlqeecSS9nnW4Ou/BC+MM/dHlbkvrEkF6OGg346Efrv7QK4JJL4Nix3tUkSXoDQ3q56XRzGMDatQa0JA2A16SXi0ajee2504C+5BKYnu5NTZKk8zKkl4N3v7t557BOffzjzqAlaYBc7h52o6Nw6lRnz/H2npJUBGfSw2pmebuTgL7gguZfrzKgJakIzqSHUaPR+fK2f/dZkopjSA+bK6+Exx/v7Dn33ONrnyWpQIb0sFjIH8bwtc+SVDRDehhEdP6cdesMaEkqnBvHlrqFBPTWrXDyZPdrkSR1Va2QjohrI+JwRExFxO4259dExH3V+YciYqxqXx8RfxERL0XE57tb+jI3OrqwgL7nHjeISdISMe9yd0SMALcD1wBHgQMRMZGZrbuTbgROZublEbEDuA24Hvgu8F+At1Zv6oaFhPOqVZ3dq1uSNHB1ZtJXAVOZ+WRmngbuBbbP6rMduKt6fD+wLSIiM1/OzL+mGdZarJk/K9mprVsNaElagupsHNsIPNNyfBS4eq4+mXk2Il4E1gPP1SkiInYBuwA2bdpU5ynLz+rVcOZM58/L7H4tkqS+KGLjWGbuy8zxzBzfsGHDoMspT0TnAb1unQEtSUtcnZA+BlzWcnxp1da2T0SsBC4CTnSjwGVt9eqFbw5z97YkLXl1lrsPAFdExBaaYbwD+MVZfSaAG4BvANcBD2Y6jVuUhYQzOHuWpCEy70w6M88CNwP7gW8DX8rMQxFxa0R8oOp2B7A+IqaATwCvvUwrIp4Cfg/4txFxNCK2dnkMw2XmD2N0yuVtSRo6te44lplfBb46q+03Wh5/F/iFOZ47toj6lpeFbg7z3tuSNJS8LWgpXN6WJM1SxO7uZW0xr302oCVpqDmTHqTRUTh1qvPnGc6StCwY0oPi8rYkaR4ud/fbu9/t7m1JUi3OpPvJ2bMkqQPOpPvFgJYkdciQ7rWIhQX0tm0GtCQtcy5395KzZ0nSIjiT7oWNGxcW0CtWGNCSpNcY0t0WAc8+2/nz7rkHXnml+/VIkpYsl7u7yeVtSVIXGdLdsNBwBgNakjQnQ3qxnD1LknrEa9KLYUBLknrIkF6Ihb722Vt7SpI64HJ3p5w9S5L6xJl0XSMjBrQkqa8M6Toi4NVXO3/e2rUGtCRpwVzuno+zZ0nSgBjSc/G1z5KkARv6kH7g4WN8dv9hjr1wipEIXslk47q1fPK9PwTAZ/cf5tkXTnFJ1fbBd3R+3+2ZSD65+kLe/5kJPvnwsebH6ULdb6hNkrRsRBY26xsfH8/JycmufKwHHj7GJ+9/lDOv1Bvj2lUjHPqv7+voQn1Wb2/+1J++1rZqRfB9F6zkhekzCwrYBx4+xqf/5DFOnXn9Xt5rV43wW//mbQa1JA2hiDiYmeOz24d6Jr3ny4/VDugnb/tZOpk/Z8v71oAGOPNqcnL6DADHXjjFp//kMYDaAfvZ/YfPCWiAU2de4bP7D7/2MRY703amLknlG9qQfuDhY7x8ut5flZoJ6Loh3W72fD4zAQvtl9dnB+axF061/TjPVu2zZ9qd/kdgIc831CWp/4Z2uftf/PaDc4bdjKnbfpaR6nGvArrV2lUjb5ghrx4JMpuz7/msW7uKR37zPXOObeO6tXx997vm/TidPr/d8nsA//yf/ABPnTjV8+Au7T8IpdXTLcM6LmkpmGu5e2hDemz3V857fiGz55n3CwnoQQher3uhVq2AVSMrmD4z/+vEe3Hd/HzX5wE+878OvXZpYd3aVdzygSt7GizDul9gWMclLRWGdIteL2/rXAGMrh5h+vQrXLJuLf/qhzfwF//7+DkzNjg3cCOar2Sb6z8aUfWZvQCxakXw2V/40Z4Fy1yrECMR/O6H6n3edjNWmOOVBn2y2NUZDRdXVdrr5dfFkObczWEGdDlWAAu4n9uc3jS6iod/4z1d/Iiv27L7K/OuTrxpdBW/+f72M/p2M9ZVKwKCczY59nsWO9e4AvjOb/9MX2pQGVxVaa/XX5e5QnrZ3Ba0dfZcJ6ATA7pfuhnQACenz/DAw8e6/FGbLlm3ttbn/+T9j7atod3O/TOv5htehdC62bAf5hpXnfFquJzv1SXL2aC+LssipBe6vL3lU39qQC9RvfrB+eR7f4i1q0bm7XfmlWxbw7PzbGZcaN/FajeutatGXluK1/Ix17+7fv57LNGgvi5D+xIs6Hx5eyluDlN7vfrBmVnW+o9fepRX5rlU1K6G873Erl3ffpkZl9chNde/0eW+qjKor0utmXREXBsRhyNiKiJ2tzm/JiLuq84/FBFjLec+XbUfjoj3dq/081vo8raz56VjZMXc39le/uB88B0b+d0P/ei8M+p2NbSbsa5aEawaOXcsg5jFfvAdG/n67nfxnd/+Gb6++10G9DLlqkp7g/q6zDuTjogR4HbgGuAocCAiJjLz8ZZuNwInM/PyiNgB3AZcHxFbgR3AlcAlwJ9HxFsys95dRhbB3dvlON/u7ovWruLl02ffcE12Zlf3urWriGhe523d6T2zOQtou5mj1z84MwF2y8QhXjh15g3nV41E2xrmmrG2azMkNQiuqrQ3qK/LvLu7I+KfAbdk5nur408DZOZvtfTZX/X5RkSsBP4vsAHY3dq3td9cn69r9+6u8UcyhmV5u9PXQ8/u37ob+YGHj53zUqjRVStYvXKEF0+daRsoF1UhutD7lMPSv8XpAw8fOyesz7e7W5LaWfBLsCLiOuDazPx31fFHgKsz8+aWPt+q+hytjp8ArgZuAf4mM++p2u8A/iwz75/1OXYBuwA2bdr040eOHFnoOFs/aL1+hb0ETZK0/BT9EqzM3JeZ45k5vmHDhn5+4v59LkmSOlQnpI8Bl7UcX1q1te1TLXdfBJyo+dzBMKAlSYWrE9IHgCsiYktErKa5EWxiVp8J4Ibq8XXAg9lcR58AdlS7v7cAVwB/253S5zFXCGca0JKkJWHe3d2ZeTYibgb2AyPAnZl5KCJuBSYzcwK4A7g7IqaA52kGOVW/LwGPA2eBf9+Pnd0txfftU0mS1G1De+9uSZKWiqI3jkmSpDcypCVJKpQhLUlSoQxpSZIKZUhLklQoQ1qSpEIZ0pIkFcqQliSpUIa0JEmFKu6OYxFxHOjC36o8x8XAc13+mKUY5rGB41vqhnl8wzw2cHz9tjkz3/BnIIsL6V6IiMl2t1sbBsM8NnB8S90wj2+YxwaOrxQud0uSVChDWpKkQi2XkN436AJ6aJjHBo5vqRvm8Q3z2MDxFWFZXJOWJGkpWi4zaUmSlpwlHdIRcW1EHI6IqYjY3eb8moi4rzr/UESMtZz7dNV+OCLe28+661ro+CJifUT8RUS8FBGf73fddS1ifNdExMGIeKx6/65+117HIsZ3VUQ8Ur09GhE/1+/a57OYn73q/Kbq3+ev9avmTiziezcWEadavn9f7HftdSzyd+ePRMQ3IuJQ9TN4QT9rr2MR37+dLd+7RyLi1Yh4e7/rP0dmLsk3YAR4AngzsBp4FNg6q89NwBerxzuA+6rHW6v+a4At1ccZGfSYuji+C4GfBD4GfH7QY+nB+N4BXFI9fitwbNDj6fL4RoGV1eN/DPy/meMS3hYztpbz9wP/E/i1QY+ny9+7MeBbgx5DD8e3Evg74Eer4/XD9LtzVp+3AU8MejxLeSZ9FTCVmU9m5mngXmD7rD7bgbuqx/cD2yIiqvZ7M/N7mfkdYKr6eCVZ8Pgy8+XM/Gvgu/0rt2OLGd/Dmfls1X4IWBsRa/pSdX2LGd90Zp6t2i8ASts4spifPSLig8B3aH7vSrSo8S0Bixnfe4C/y8xHATLzRGa+0qe66+rW9+/D1XMHaimH9EbgmZbjo1Vb2z7VL70Xaf7Pr85zB20x41sKujW+nwe+mZnf61GdC7Wo8UXE1RFxCHgM+FhLaJdgwWOLiO8DPgV8pg91LtRi/21uiYiHI+IvI+KdvS52ARYzvrcAGRH7I+KbEfHrfai3U9363XI98Mc9qrG2lYMuQFqoiLgSuI3m/+6HSmY+BFwZEf8UuCsi/iwzS14ZqesW4HOZ+dLSmXh25O+BTZl5IiJ+HHggIq7MzH8YdGFdspLmpbSfAKaBr0XEwcz82mDL6q6IuBqYzsxvDbqWpTyTPgZc1nJ8adXWtk9ErAQuAk7UfO6gLWZ8S8GixhcRlwJfBn45M5/oebWd68r3LzO/DbxE89p7KRYztquB34mIp4D/APyniLi51wV3aMHjqy6hnQDIzIM0r42+pecVd2Yx37+jwF9l5nOZOQ18FfixnlfcmW787O2ggFk0LO2QPgBcERFbImI1zS/qxKw+E8AN1ePrgAezuSNgAthR7fDbAlwB/G2f6q5rMeNbChY8vohYB3wF2J2ZX+9bxZ1ZzPi2VL84iIjNwA8DT/Wn7FoWPLbMfGdmjmXmGPDfgf+WmaW9AmEx37sNETECEBFvpvm75ck+1V3XYn637AfeFhGj1b/RnwYe71PddS3qd2dErAA+RAHXo4Glu7u7+nr+a+D/0Pzf6p6q7VbgA9XjC2juIJ2iGcJvbnnunup5h4H3DXosPRjfU8DzNGdhR5m1u7GEt4WOD/jPwMvAIy1vPzjo8XRxfB+huanqEeCbwAcHPZZu/tts+Ri3UODu7kV+735+1vfu/YMeS7e/f8AvVWP8FvA7gx5LD8b3L4G/GfQYZt6845gkSYVaysvdkiQNNUNakqRCGdKSJBXKkJYkqVCGtCRJhTKkJUkqlCEtSVKhDGlJkgr1/wFZwUp+c2A+BgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test[:,1], pred[:,1])\n",
    "plt.scatter(y_test[:,1], y_test[:,1], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "gZaH-qCDhIu1",
    "outputId": "75b165d8-dd69-47eb-f458-e126249139bd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff640d61f10>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test[:,2], pred[:,2])\n",
    "plt.scatter(y_test[:,2], y_test[:,2], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "gRfTKP8phIu2",
    "outputId": "4e64e29e-6581-4260-b805-1f518fc57432"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7ff6433ef390>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test[:,3], pred[:,3])\n",
    "plt.scatter(y_test[:,3], y_test[:,3], color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optModel(a=8,b=8):\n",
    "    bestLoss = 1e12\n",
    "    bestA = -1\n",
    "    bestB = -1\n",
    "    for i in range(a):\n",
    "        for j in range(b):\n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Dense(np.shape(X)[1],activation='relu'))\n",
    "\n",
    "            for i in range(2+i):\n",
    "                model.add(Dense(np.shape(X)[1],activation='relu'))\n",
    "                model.add(Dropout(1/(2+j)))\n",
    "\n",
    "            model.add(Dense(4))\n",
    "\n",
    "            model.compile(optimizer='adam',loss='mse')\n",
    "\n",
    "            early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "        model.fit(x=X_train, \n",
    "                  y=y_train, \n",
    "                  epochs=150, \n",
    "                    batch_size=256, \n",
    "                    validation_data=(X_test, y_test), \n",
    "                    callbacks=[early_stop]\n",
    "                  )\n",
    "        l = pd.DataFrame(model.history.history).iloc[0,-1]\n",
    "        if l < bestLoss:\n",
    "            bestLoss = l\n",
    "            bestA = i\n",
    "            bestB = j\n",
    "    \n",
    "    return bestLoss, bestA, bestB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 48.9470 - val_loss: 40.9405\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 42.5024 - val_loss: 40.8058\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.3176 - val_loss: 40.3759\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.9090 - val_loss: 39.9191\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.4645 - val_loss: 39.6769\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.3352 - val_loss: 39.5973\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.1221 - val_loss: 39.4363\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.1325 - val_loss: 39.4883\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9725 - val_loss: 39.3100\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9098 - val_loss: 39.1335\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8039 - val_loss: 38.9017\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7058 - val_loss: 38.8422\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5634 - val_loss: 38.8053\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4069 - val_loss: 38.7090\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5320 - val_loss: 38.6765\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4276 - val_loss: 38.8467\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5504 - val_loss: 38.5653\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4824 - val_loss: 38.5857\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3236 - val_loss: 38.6887\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3478 - val_loss: 38.5564\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0992 - val_loss: 38.6273\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1592 - val_loss: 38.5500\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1954 - val_loss: 38.5048\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2006 - val_loss: 38.5905\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1532 - val_loss: 38.5340\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0841 - val_loss: 38.5282\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0510 - val_loss: 38.4162\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0611 - val_loss: 38.4953\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9984 - val_loss: 38.4301\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0576 - val_loss: 38.5954\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9256 - val_loss: 38.4752\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8089 - val_loss: 38.5462\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9874 - val_loss: 38.5093\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8769 - val_loss: 38.7129\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9090 - val_loss: 38.4582\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8621 - val_loss: 38.4713\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7670 - val_loss: 38.4894\n",
      "Epoch 00037: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 46.2609 - val_loss: 39.8563\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.3335 - val_loss: 39.6432\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.2284 - val_loss: 39.5604\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7983 - val_loss: 39.2481\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6500 - val_loss: 39.0002\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5845 - val_loss: 38.7768\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4955 - val_loss: 38.6498\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3841 - val_loss: 38.6546\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4082 - val_loss: 38.6653\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3361 - val_loss: 38.7513\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3108 - val_loss: 38.5561\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1697 - val_loss: 38.4718\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0660 - val_loss: 38.5603\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0718 - val_loss: 38.5874\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1590 - val_loss: 38.4353\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0303 - val_loss: 38.6769\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0535 - val_loss: 38.5295\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9228 - val_loss: 38.4817\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9129 - val_loss: 38.6693\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8836 - val_loss: 38.5670\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8198 - val_loss: 38.4944\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8862 - val_loss: 38.6714\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9251 - val_loss: 38.6212\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7470 - val_loss: 38.6459\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7737 - val_loss: 38.6302\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 47.0197 - val_loss: 40.4982\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.8689 - val_loss: 39.6010\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9736 - val_loss: 39.5516\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6925 - val_loss: 39.0261\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4193 - val_loss: 38.7490\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2857 - val_loss: 38.7194\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0635 - val_loss: 38.6089\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1011 - val_loss: 38.5324\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0782 - val_loss: 38.6225\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9928 - val_loss: 38.6789\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1093 - val_loss: 38.9595\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0097 - val_loss: 38.3203\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9773 - val_loss: 38.5599\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8788 - val_loss: 38.5698\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9801 - val_loss: 38.6046\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9376 - val_loss: 38.2894\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8565 - val_loss: 38.3415\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8371 - val_loss: 38.3986\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8058 - val_loss: 38.2720\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9066 - val_loss: 38.4351\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8023 - val_loss: 38.3489\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7604 - val_loss: 38.4368\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7162 - val_loss: 38.5424\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7438 - val_loss: 38.5078\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6461 - val_loss: 38.3367\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6147 - val_loss: 38.3475\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7283 - val_loss: 38.3311\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6926 - val_loss: 38.3023\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6981 - val_loss: 38.4539\n",
      "Epoch 00029: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 45.5383 - val_loss: 39.6762\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1515 - val_loss: 39.1961\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6539 - val_loss: 39.0516\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4744 - val_loss: 38.8667\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2172 - val_loss: 38.6480\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0860 - val_loss: 38.5586\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0092 - val_loss: 38.8547\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9355 - val_loss: 38.5639\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8590 - val_loss: 38.4820\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7147 - val_loss: 38.4702\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8639 - val_loss: 38.5842\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7557 - val_loss: 38.5213\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7151 - val_loss: 38.5536\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7118 - val_loss: 38.6048\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7198 - val_loss: 38.4449\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7424 - val_loss: 38.6308\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7357 - val_loss: 38.7896\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6630 - val_loss: 38.6651\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6391 - val_loss: 38.4314\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6748 - val_loss: 38.2901\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5422 - val_loss: 38.4480\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5471 - val_loss: 38.4694\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6060 - val_loss: 38.6105\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5293 - val_loss: 38.4954\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4752 - val_loss: 38.3913\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5445 - val_loss: 38.2652\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4405 - val_loss: 38.3673\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4100 - val_loss: 38.3186\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4249 - val_loss: 38.5870\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4961 - val_loss: 38.2948\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4703 - val_loss: 38.3233\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4265 - val_loss: 38.7117\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5526 - val_loss: 38.3956\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4705 - val_loss: 38.2223\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4910 - val_loss: 38.3758\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4530 - val_loss: 38.3896\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.3450 - val_loss: 38.2171\n",
      "Epoch 38/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4787 - val_loss: 38.5551\n",
      "Epoch 39/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4765 - val_loss: 38.2705\n",
      "Epoch 40/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.3722 - val_loss: 38.3196\n",
      "Epoch 41/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4190 - val_loss: 38.3832\n",
      "Epoch 42/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.3712 - val_loss: 38.2844\n",
      "Epoch 43/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4130 - val_loss: 38.3422\n",
      "Epoch 44/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.3614 - val_loss: 38.2856\n",
      "Epoch 45/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4454 - val_loss: 38.5598\n",
      "Epoch 46/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.3102 - val_loss: 38.4319\n",
      "Epoch 47/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.2638 - val_loss: 38.3117\n",
      "Epoch 00047: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 44.7111 - val_loss: 40.7368\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1098 - val_loss: 39.4460\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5372 - val_loss: 39.3618\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2911 - val_loss: 38.8537\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2063 - val_loss: 38.5728\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0435 - val_loss: 38.9852\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9642 - val_loss: 38.7864\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9376 - val_loss: 38.8206\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8858 - val_loss: 38.5852\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8826 - val_loss: 38.4525\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8158 - val_loss: 38.4790\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8138 - val_loss: 38.4442\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7647 - val_loss: 38.4974\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7679 - val_loss: 38.3162\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7865 - val_loss: 38.9427\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7190 - val_loss: 38.4437\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6852 - val_loss: 38.4231\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6222 - val_loss: 38.8831\n",
      "Epoch 19/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5835 - val_loss: 38.6721\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6434 - val_loss: 38.3121\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7137 - val_loss: 38.5225\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6113 - val_loss: 38.7124\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5519 - val_loss: 38.3716\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6069 - val_loss: 38.6123\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5856 - val_loss: 38.4822\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6079 - val_loss: 38.5820\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4470 - val_loss: 38.4866\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5720 - val_loss: 38.5999\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5225 - val_loss: 38.5513\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4593 - val_loss: 38.4754\n",
      "Epoch 00030: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 46.3286 - val_loss: 40.4076\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1935 - val_loss: 39.7040\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7359 - val_loss: 39.1799\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3510 - val_loss: 39.0869\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3931 - val_loss: 39.1426\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1872 - val_loss: 38.9504\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0776 - val_loss: 38.9244\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9417 - val_loss: 38.8401\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9463 - val_loss: 38.5513\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9558 - val_loss: 38.8564\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7863 - val_loss: 38.4715\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8395 - val_loss: 38.7721\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7393 - val_loss: 38.6497\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7514 - val_loss: 38.4032\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6320 - val_loss: 38.6054\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7409 - val_loss: 39.0457\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6450 - val_loss: 39.0547\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7034 - val_loss: 38.5435\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7132 - val_loss: 38.6337\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7066 - val_loss: 38.6140\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6194 - val_loss: 38.3907\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5630 - val_loss: 38.5495\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6991 - val_loss: 38.8177\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5453 - val_loss: 38.3686\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6222 - val_loss: 38.7445\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5804 - val_loss: 38.5197\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5217 - val_loss: 39.2619\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5924 - val_loss: 38.4300\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5867 - val_loss: 38.3623\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5415 - val_loss: 38.5043\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4953 - val_loss: 38.9182\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6348 - val_loss: 38.6308\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5650 - val_loss: 38.6261\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5177 - val_loss: 38.6325\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5604 - val_loss: 38.6958\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5224 - val_loss: 38.5254\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4613 - val_loss: 38.4582\n",
      "Epoch 38/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4768 - val_loss: 38.4670\n",
      "Epoch 39/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4585 - val_loss: 38.6765\n",
      "Epoch 00039: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 44.9418 - val_loss: 40.6891\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0722 - val_loss: 40.4710\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7226 - val_loss: 40.4684\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3045 - val_loss: 39.7333\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1552 - val_loss: 39.2930\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0618 - val_loss: 38.9589\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8863 - val_loss: 39.1070\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9460 - val_loss: 38.8136\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8799 - val_loss: 38.8141\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8359 - val_loss: 39.0166\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7530 - val_loss: 38.7861\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7261 - val_loss: 38.8561\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7731 - val_loss: 38.7461\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6871 - val_loss: 38.4493\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6591 - val_loss: 38.5109\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6376 - val_loss: 38.4301\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6320 - val_loss: 38.6492\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6248 - val_loss: 38.3354\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5631 - val_loss: 38.4955\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6588 - val_loss: 38.6944\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5670 - val_loss: 38.4352\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5050 - val_loss: 38.7285\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5252 - val_loss: 38.4662\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4954 - val_loss: 38.4976\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5073 - val_loss: 38.6697\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5042 - val_loss: 38.6818\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4740 - val_loss: 38.6003\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4697 - val_loss: 38.3570\n",
      "Epoch 00028: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 44.6385 - val_loss: 40.9804\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1355 - val_loss: 41.2347\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5475 - val_loss: 40.1393\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1519 - val_loss: 38.9087\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9947 - val_loss: 38.9299\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9360 - val_loss: 38.8722\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8383 - val_loss: 38.6115\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7940 - val_loss: 38.9602\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7391 - val_loss: 38.5102\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7256 - val_loss: 38.8308\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6628 - val_loss: 38.5949\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6141 - val_loss: 39.0516\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6287 - val_loss: 38.8763\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5839 - val_loss: 38.4055\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6165 - val_loss: 38.5890\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5971 - val_loss: 38.6955\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5777 - val_loss: 38.5962\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5832 - val_loss: 38.7810\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5005 - val_loss: 38.6012\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5732 - val_loss: 38.6061\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5119 - val_loss: 38.7252\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4482 - val_loss: 38.3872\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5789 - val_loss: 38.4853\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5040 - val_loss: 38.4498\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.3720 - val_loss: 38.8393\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5197 - val_loss: 38.5560\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4411 - val_loss: 38.4227\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4014 - val_loss: 38.3971\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4887 - val_loss: 38.7274\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4799 - val_loss: 38.5740\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4662 - val_loss: 38.2808\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4506 - val_loss: 38.5847\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4403 - val_loss: 38.5471\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4243 - val_loss: 38.5272\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4248 - val_loss: 38.6641\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.3714 - val_loss: 38.6064\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4215 - val_loss: 38.5282\n",
      "Epoch 38/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.3239 - val_loss: 38.8421\n",
      "Epoch 39/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4075 - val_loss: 38.7618\n",
      "Epoch 40/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.2568 - val_loss: 38.4044\n",
      "Epoch 41/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4137 - val_loss: 38.5627\n",
      "Epoch 00041: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 49.4385 - val_loss: 41.1089\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 42.8317 - val_loss: 40.2948\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.3958 - val_loss: 39.5425\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.9258 - val_loss: 39.4624\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.7190 - val_loss: 39.1328\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.4531 - val_loss: 39.2970\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.4432 - val_loss: 39.2349\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.3774 - val_loss: 38.8013\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.2628 - val_loss: 39.0772\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.0830 - val_loss: 39.2101\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9637 - val_loss: 38.8364\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9063 - val_loss: 38.7167\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9264 - val_loss: 38.7859\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7200 - val_loss: 38.6083\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5926 - val_loss: 38.9584\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7783 - val_loss: 39.0043\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5965 - val_loss: 38.7236\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5389 - val_loss: 38.6279\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3782 - val_loss: 38.6244\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5868 - val_loss: 38.8673\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2635 - val_loss: 38.5281\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2892 - val_loss: 38.8427\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2078 - val_loss: 38.6845\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2923 - val_loss: 38.9148\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2430 - val_loss: 38.8118\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2232 - val_loss: 38.8094\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0749 - val_loss: 38.8356\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0850 - val_loss: 38.6280\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1321 - val_loss: 38.5496\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1233 - val_loss: 38.8882\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0826 - val_loss: 38.9487\n",
      "Epoch 00031: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 46.8512 - val_loss: 40.6967\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.0420 - val_loss: 39.8994\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.2277 - val_loss: 39.3365\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7455 - val_loss: 39.2635\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7715 - val_loss: 39.1862\n",
      "Epoch 6/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7004 - val_loss: 39.2501\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6043 - val_loss: 38.9345\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5326 - val_loss: 39.0531\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3635 - val_loss: 38.8775\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4704 - val_loss: 38.9683\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1902 - val_loss: 38.9074\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1806 - val_loss: 38.9393\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2750 - val_loss: 38.8300\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1364 - val_loss: 38.6510\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0053 - val_loss: 38.7782\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1269 - val_loss: 38.5689\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1098 - val_loss: 39.1210\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0359 - val_loss: 38.7415\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9590 - val_loss: 38.7245\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9754 - val_loss: 38.7770\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9738 - val_loss: 38.7127\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8911 - val_loss: 38.5807\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8623 - val_loss: 38.4640\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8839 - val_loss: 38.6222\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8121 - val_loss: 38.7640\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7422 - val_loss: 38.5118\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8712 - val_loss: 38.6923\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8828 - val_loss: 38.7285\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7516 - val_loss: 38.7088\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8130 - val_loss: 38.9030\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7538 - val_loss: 38.6543\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7961 - val_loss: 38.5956\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7355 - val_loss: 38.6368\n",
      "Epoch 00033: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 46.1869 - val_loss: 40.8206\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.7020 - val_loss: 40.8687\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.0530 - val_loss: 39.4101\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8283 - val_loss: 39.1700\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5570 - val_loss: 38.7130\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3780 - val_loss: 38.7696\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3758 - val_loss: 38.7040\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 16s 64ms/step - loss: 39.2522 - val_loss: 38.6539\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1105 - val_loss: 38.7498\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1150 - val_loss: 38.5861\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1133 - val_loss: 38.7701\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.1105 - val_loss: 38.4531\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 39.0571 - val_loss: 38.9490\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0366 - val_loss: 38.4126\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0103 - val_loss: 38.4685\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.8458 - val_loss: 38.3616\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 38.7668 - val_loss: 38.3189\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 39.0010 - val_loss: 38.4507\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.9297 - val_loss: 38.8034\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0336 - val_loss: 39.0452\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.8324 - val_loss: 38.4388\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7607 - val_loss: 38.5780\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7260 - val_loss: 38.5561\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7543 - val_loss: 39.0430\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8987 - val_loss: 38.5982\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8046 - val_loss: 38.6734\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8460 - val_loss: 38.7360\n",
      "Epoch 00027: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 45.2198 - val_loss: 40.2187\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 40.0840 - val_loss: 39.3328\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 39.6099 - val_loss: 39.4620\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.3985 - val_loss: 38.9939\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1148 - val_loss: 38.9744\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0284 - val_loss: 38.6206\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8787 - val_loss: 38.8597\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9330 - val_loss: 38.6769\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7848 - val_loss: 38.8080\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7122 - val_loss: 38.9045\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7312 - val_loss: 38.3583\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6789 - val_loss: 38.4563\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6758 - val_loss: 38.3263\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6467 - val_loss: 38.4884\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6995 - val_loss: 38.5159\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6432 - val_loss: 38.4731\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6133 - val_loss: 38.5913\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5796 - val_loss: 38.3734\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5919 - val_loss: 38.3516\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5816 - val_loss: 38.5320\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6050 - val_loss: 38.4272\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5175 - val_loss: 38.8328\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5865 - val_loss: 38.5566\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 45.0715 - val_loss: 41.2562\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 40.3248 - val_loss: 40.0835\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6597 - val_loss: 39.4341\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4329 - val_loss: 39.4397\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2370 - val_loss: 39.3938\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1399 - val_loss: 38.9369\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0243 - val_loss: 39.2749\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8252 - val_loss: 38.9662\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8795 - val_loss: 38.6860\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8630 - val_loss: 38.8396\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8319 - val_loss: 38.6493\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7616 - val_loss: 38.8316\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7352 - val_loss: 38.8208\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7352 - val_loss: 38.7180\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6519 - val_loss: 38.7688\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6550 - val_loss: 38.6522\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5615 - val_loss: 38.9690\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7154 - val_loss: 38.6575\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6369 - val_loss: 39.0024\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6560 - val_loss: 38.8779\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6280 - val_loss: 38.7343\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 45.0630 - val_loss: 41.0701\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0016 - val_loss: 39.8904\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5366 - val_loss: 39.5539\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2598 - val_loss: 39.0946\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1110 - val_loss: 38.9272\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9995 - val_loss: 38.7860\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9199 - val_loss: 38.6719\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7800 - val_loss: 38.8392\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8036 - val_loss: 38.6332\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7567 - val_loss: 38.5065\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7189 - val_loss: 38.5349\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6296 - val_loss: 39.2215\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7097 - val_loss: 38.8776\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6344 - val_loss: 38.8870\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6746 - val_loss: 38.5728\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6270 - val_loss: 38.7044\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6647 - val_loss: 38.5745\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5939 - val_loss: 38.5555\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5493 - val_loss: 38.8682\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4989 - val_loss: 38.6488\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 43.9779 - val_loss: 40.0311\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7474 - val_loss: 39.6791\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4213 - val_loss: 39.3313\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1370 - val_loss: 38.9283\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1498 - val_loss: 38.8573\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9784 - val_loss: 39.0777\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9041 - val_loss: 39.1759\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8320 - val_loss: 38.7220\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8120 - val_loss: 38.6977\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7743 - val_loss: 38.8226\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7623 - val_loss: 38.7561\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7022 - val_loss: 38.5537\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6954 - val_loss: 38.6451\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7063 - val_loss: 38.6819\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6592 - val_loss: 38.7690\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6017 - val_loss: 38.8839\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6570 - val_loss: 38.9313\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6239 - val_loss: 38.6767\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5085 - val_loss: 38.6868\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5206 - val_loss: 38.8967\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6182 - val_loss: 38.7763\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5134 - val_loss: 39.2883\n",
      "Epoch 00022: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.0031 - val_loss: 40.8845\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7715 - val_loss: 39.8321\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4312 - val_loss: 39.5322\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1762 - val_loss: 39.6562\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0080 - val_loss: 38.8528\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9505 - val_loss: 39.2120\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8690 - val_loss: 38.9031\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8153 - val_loss: 38.8629\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7855 - val_loss: 38.9796\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7538 - val_loss: 38.8724\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6744 - val_loss: 38.8615\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6493 - val_loss: 39.2242\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7368 - val_loss: 39.1422\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6298 - val_loss: 38.7186\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5640 - val_loss: 38.6303\n",
      "Epoch 16/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6715 - val_loss: 38.5613\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6194 - val_loss: 39.0000\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5616 - val_loss: 38.9141\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5735 - val_loss: 38.8255\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5382 - val_loss: 38.6564\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5281 - val_loss: 38.5297\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5386 - val_loss: 38.6564\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5775 - val_loss: 38.6687\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5173 - val_loss: 38.3183\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5210 - val_loss: 39.0348\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4950 - val_loss: 39.2941\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5346 - val_loss: 38.8642\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5260 - val_loss: 39.1132\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4797 - val_loss: 38.8953\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.3999 - val_loss: 38.9626\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4874 - val_loss: 38.9955\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.3778 - val_loss: 38.6945\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.3946 - val_loss: 38.8017\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4559 - val_loss: 38.6806\n",
      "Epoch 00034: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 48.6261 - val_loss: 42.6710\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 42.1756 - val_loss: 39.6241\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.0088 - val_loss: 39.5994\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.7766 - val_loss: 39.2794\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.4465 - val_loss: 39.2198\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.2661 - val_loss: 39.1688\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.1061 - val_loss: 39.1248\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9206 - val_loss: 38.7745\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9161 - val_loss: 39.1690\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7876 - val_loss: 38.8358\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6999 - val_loss: 38.8608\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6070 - val_loss: 38.7933\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5810 - val_loss: 38.9174\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5040 - val_loss: 38.7163\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4128 - val_loss: 38.5729\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4751 - val_loss: 39.0713\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4343 - val_loss: 38.8878\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2730 - val_loss: 38.6573\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3417 - val_loss: 38.7154\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2547 - val_loss: 38.8858\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3258 - val_loss: 38.5166\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2162 - val_loss: 38.7937\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2086 - val_loss: 38.7165\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2306 - val_loss: 38.9139\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1960 - val_loss: 38.6907\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1179 - val_loss: 38.5054\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1516 - val_loss: 38.5422\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1259 - val_loss: 38.6236\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1232 - val_loss: 38.5592\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0887 - val_loss: 38.6207\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0565 - val_loss: 38.5986\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9097 - val_loss: 38.5760\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0541 - val_loss: 38.4260\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9924 - val_loss: 38.9771\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9529 - val_loss: 38.6991\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8932 - val_loss: 38.5640\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0240 - val_loss: 38.6191\n",
      "Epoch 38/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9441 - val_loss: 38.4808\n",
      "Epoch 39/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8085 - val_loss: 38.7091\n",
      "Epoch 40/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9020 - val_loss: 38.3582\n",
      "Epoch 41/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8510 - val_loss: 38.7130\n",
      "Epoch 42/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8734 - val_loss: 38.4939\n",
      "Epoch 43/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8153 - val_loss: 38.7146\n",
      "Epoch 44/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8385 - val_loss: 38.6157\n",
      "Epoch 45/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9460 - val_loss: 38.5726\n",
      "Epoch 46/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8811 - val_loss: 38.8376\n",
      "Epoch 47/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7830 - val_loss: 38.5072\n",
      "Epoch 48/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7640 - val_loss: 38.6613\n",
      "Epoch 49/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7128 - val_loss: 38.5126\n",
      "Epoch 50/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7756 - val_loss: 38.8016\n",
      "Epoch 00050: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 47.1523 - val_loss: 40.8373\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.9378 - val_loss: 39.6282\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.2258 - val_loss: 39.2825\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8647 - val_loss: 38.9721\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6124 - val_loss: 39.0294\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4407 - val_loss: 38.8903\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5292 - val_loss: 38.9945\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3412 - val_loss: 38.9611\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2568 - val_loss: 39.0949\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1983 - val_loss: 38.9178\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2722 - val_loss: 38.6503\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3154 - val_loss: 38.7933\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1961 - val_loss: 38.6989\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1868 - val_loss: 38.5689\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0442 - val_loss: 38.7323\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0699 - val_loss: 38.7294\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0114 - val_loss: 38.6908\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9821 - val_loss: 38.5294\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9716 - val_loss: 38.6232\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8329 - val_loss: 38.7924\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9989 - val_loss: 38.8265\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8501 - val_loss: 38.5006\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8463 - val_loss: 38.6784\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9148 - val_loss: 38.8364\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8042 - val_loss: 38.5497\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9123 - val_loss: 38.6731\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7985 - val_loss: 38.8298\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7723 - val_loss: 38.9030\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7493 - val_loss: 38.6075\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7388 - val_loss: 38.6041\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8056 - val_loss: 38.5412\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8408 - val_loss: 39.0178\n",
      "Epoch 00032: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 46.3781 - val_loss: 39.8358\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.4473 - val_loss: 39.4652\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9060 - val_loss: 39.1422\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4916 - val_loss: 38.7121\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5353 - val_loss: 38.9203\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5403 - val_loss: 38.8067\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3787 - val_loss: 39.3801\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2915 - val_loss: 38.7770\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3304 - val_loss: 38.6537\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2748 - val_loss: 38.8613\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0879 - val_loss: 38.8533\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0452 - val_loss: 38.5402\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0219 - val_loss: 38.6560\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1048 - val_loss: 38.6753\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1003 - val_loss: 38.5111\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0552 - val_loss: 39.0934\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0377 - val_loss: 38.6102\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9105 - val_loss: 39.0845\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8946 - val_loss: 38.7430\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9453 - val_loss: 38.4958\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8481 - val_loss: 39.0178\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8814 - val_loss: 39.0173\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8348 - val_loss: 38.5333\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8878 - val_loss: 38.5571\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8844 - val_loss: 38.6796\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7342 - val_loss: 38.7041\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7108 - val_loss: 38.6171\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7703 - val_loss: 38.6078\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6647 - val_loss: 38.6118\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6820 - val_loss: 38.5588\n",
      "Epoch 00030: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 44.2320 - val_loss: 40.7121\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.2095 - val_loss: 39.3649\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5713 - val_loss: 39.3734\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3043 - val_loss: 39.0256\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0798 - val_loss: 38.6427\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1312 - val_loss: 38.8508\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0191 - val_loss: 38.6040\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9129 - val_loss: 38.6136\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8889 - val_loss: 39.0173\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8736 - val_loss: 38.7579\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9302 - val_loss: 38.6605\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8855 - val_loss: 38.6090\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8989 - val_loss: 38.9579\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7872 - val_loss: 38.5006\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7902 - val_loss: 38.6983\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7639 - val_loss: 38.6513\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7911 - val_loss: 38.4255\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7330 - val_loss: 38.6508\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7445 - val_loss: 38.4243\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7482 - val_loss: 38.6422\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7477 - val_loss: 39.0645\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6494 - val_loss: 38.4823\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6416 - val_loss: 38.6011\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5409 - val_loss: 38.9787\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5994 - val_loss: 38.4675\n",
      "Epoch 26/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5928 - val_loss: 38.4927\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5632 - val_loss: 38.5820\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6233 - val_loss: 38.4013\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5959 - val_loss: 38.7830\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5725 - val_loss: 38.8217\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5692 - val_loss: 38.5573\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5457 - val_loss: 38.4838\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5202 - val_loss: 39.0469\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5926 - val_loss: 38.4041\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5312 - val_loss: 38.3054\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.5496 - val_loss: 38.5097\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4940 - val_loss: 38.4370\n",
      "Epoch 38/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4451 - val_loss: 38.6075\n",
      "Epoch 39/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4500 - val_loss: 38.7904\n",
      "Epoch 40/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4659 - val_loss: 38.4748\n",
      "Epoch 41/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4341 - val_loss: 38.5472\n",
      "Epoch 42/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.3981 - val_loss: 38.4554\n",
      "Epoch 43/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4757 - val_loss: 38.5597\n",
      "Epoch 44/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4446 - val_loss: 38.9694\n",
      "Epoch 45/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.4884 - val_loss: 38.6998\n",
      "Epoch 00045: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 45.2754 - val_loss: 40.1136\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1903 - val_loss: 39.5720\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5535 - val_loss: 38.8896\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3418 - val_loss: 38.7411\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3159 - val_loss: 38.7629\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0271 - val_loss: 38.5741\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9997 - val_loss: 38.6364\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9284 - val_loss: 38.5863\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9199 - val_loss: 38.6495\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8394 - val_loss: 38.4183\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9149 - val_loss: 38.6048\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8536 - val_loss: 38.4802\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7655 - val_loss: 38.8941\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8130 - val_loss: 38.4979\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7439 - val_loss: 38.4311\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7692 - val_loss: 38.6145\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7246 - val_loss: 38.4762\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7725 - val_loss: 38.4338\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7802 - val_loss: 38.5854\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7171 - val_loss: 38.4517\n",
      "Epoch 00020: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 45.3692 - val_loss: 40.8422\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1154 - val_loss: 39.9865\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5364 - val_loss: 39.1210\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3014 - val_loss: 39.3477\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2144 - val_loss: 38.7774\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1946 - val_loss: 39.2810\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0394 - val_loss: 39.4944\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0051 - val_loss: 39.2168\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8399 - val_loss: 38.7216\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8172 - val_loss: 38.9562\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8460 - val_loss: 38.5714\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8067 - val_loss: 38.6133\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7482 - val_loss: 38.6810\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7521 - val_loss: 39.0399\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7503 - val_loss: 39.4134\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6643 - val_loss: 38.5695\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5964 - val_loss: 38.6810\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7250 - val_loss: 38.7219\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6227 - val_loss: 38.6287\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6010 - val_loss: 38.7123\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6113 - val_loss: 39.0369\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6255 - val_loss: 38.7517\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6399 - val_loss: 38.5185\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6162 - val_loss: 38.8721\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5543 - val_loss: 38.5886\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5639 - val_loss: 38.5344\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5737 - val_loss: 38.3184\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5795 - val_loss: 38.5682\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5716 - val_loss: 38.5703\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5407 - val_loss: 38.6863\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5665 - val_loss: 38.8254\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4973 - val_loss: 38.7601\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4686 - val_loss: 38.4656\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4545 - val_loss: 38.5458\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4645 - val_loss: 38.6384\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4713 - val_loss: 38.4627\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4331 - val_loss: 38.5688\n",
      "Epoch 00037: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.6595 - val_loss: 41.1846\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0912 - val_loss: 40.1881\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7903 - val_loss: 39.3435\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3880 - val_loss: 38.9986\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2528 - val_loss: 39.1884\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2221 - val_loss: 39.1275\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1400 - val_loss: 39.2266\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0062 - val_loss: 38.9617\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0096 - val_loss: 38.8325\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9805 - val_loss: 38.6159\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0019 - val_loss: 38.6641\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9374 - val_loss: 38.5483\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8778 - val_loss: 38.8960\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8750 - val_loss: 38.8435\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8943 - val_loss: 38.6524\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8526 - val_loss: 38.9123\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8200 - val_loss: 39.0531\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7578 - val_loss: 38.8957\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8071 - val_loss: 38.6301\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7317 - val_loss: 39.4218\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7979 - val_loss: 38.7824\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6971 - val_loss: 38.4586\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6615 - val_loss: 38.3859\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7682 - val_loss: 38.5691\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7609 - val_loss: 38.5539\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6370 - val_loss: 39.0983\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6290 - val_loss: 38.6435\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6381 - val_loss: 38.8090\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6439 - val_loss: 39.0098\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5868 - val_loss: 38.4027\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6105 - val_loss: 38.5461\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6222 - val_loss: 38.5876\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5818 - val_loss: 38.8863\n",
      "Epoch 00033: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 46.3122 - val_loss: 40.0389\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7445 - val_loss: 39.2031\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2743 - val_loss: 39.4574\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0217 - val_loss: 39.6156\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0567 - val_loss: 38.9648\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9126 - val_loss: 39.2967\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9097 - val_loss: 38.8236\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8151 - val_loss: 38.5304\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6398 - val_loss: 39.0580\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7145 - val_loss: 38.6172\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6968 - val_loss: 38.5928\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6284 - val_loss: 38.6743\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6404 - val_loss: 38.6475\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6069 - val_loss: 38.5112\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6390 - val_loss: 38.8259\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5842 - val_loss: 38.4668\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5195 - val_loss: 38.6979\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5038 - val_loss: 38.8196\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4963 - val_loss: 38.5681\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6094 - val_loss: 38.8246\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6067 - val_loss: 38.5493\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5274 - val_loss: 38.6464\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4663 - val_loss: 38.5340\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5227 - val_loss: 38.6561\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4934 - val_loss: 38.9405\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4168 - val_loss: 38.6704\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 48.1389 - val_loss: 41.9165\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 42.1395 - val_loss: 40.0288\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.1706 - val_loss: 39.7923\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.8477 - val_loss: 39.6672\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.6411 - val_loss: 39.6075\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.5320 - val_loss: 39.4921\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.4255 - val_loss: 39.7050\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.1170 - val_loss: 39.5583\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.1658 - val_loss: 39.6627\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.1268 - val_loss: 39.5653\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.0671 - val_loss: 39.5272\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9726 - val_loss: 39.3641\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8825 - val_loss: 39.3191\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8539 - val_loss: 39.4469\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7034 - val_loss: 39.3385\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7705 - val_loss: 39.6837\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6580 - val_loss: 39.4491\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6904 - val_loss: 39.5569\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6292 - val_loss: 39.4269\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6814 - val_loss: 39.5595\n",
      "Epoch 21/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5280 - val_loss: 39.4144\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4104 - val_loss: 39.4411\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3859 - val_loss: 39.5196\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 46.4226 - val_loss: 40.7904\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.8388 - val_loss: 39.9802\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.3337 - val_loss: 39.4579\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9664 - val_loss: 39.1365\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8859 - val_loss: 39.5039\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7407 - val_loss: 39.1767\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7170 - val_loss: 39.2232\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5822 - val_loss: 39.5427\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5982 - val_loss: 39.3607\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5378 - val_loss: 38.9784\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3040 - val_loss: 38.9474\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3772 - val_loss: 39.3125\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4099 - val_loss: 39.1860\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2637 - val_loss: 39.1853\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3064 - val_loss: 38.9098\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2759 - val_loss: 39.0564\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2557 - val_loss: 39.4636\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2110 - val_loss: 39.0989\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1402 - val_loss: 39.1694\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0253 - val_loss: 39.0252\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0311 - val_loss: 39.2176\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9760 - val_loss: 39.0151\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9802 - val_loss: 39.0489\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9388 - val_loss: 39.0445\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7962 - val_loss: 39.0302\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 45.5985 - val_loss: 40.3895\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.6103 - val_loss: 40.2026\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9295 - val_loss: 39.6134\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6623 - val_loss: 39.4179\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5297 - val_loss: 39.4223\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4025 - val_loss: 39.1964\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3401 - val_loss: 38.7536\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3204 - val_loss: 38.9727\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1429 - val_loss: 38.9692\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1798 - val_loss: 39.1635\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0811 - val_loss: 38.8345\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2187 - val_loss: 39.1224\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0453 - val_loss: 38.7960\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0264 - val_loss: 38.6832\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8822 - val_loss: 38.9164\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0792 - val_loss: 38.7944\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8703 - val_loss: 38.8400\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8914 - val_loss: 38.8335\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9645 - val_loss: 39.0280\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.0405 - val_loss: 39.3460\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8271 - val_loss: 38.6746\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7468 - val_loss: 38.9921\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8642 - val_loss: 38.9747\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8793 - val_loss: 39.0230\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8177 - val_loss: 38.9795\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7049 - val_loss: 39.0710\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8369 - val_loss: 38.9355\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8073 - val_loss: 38.7418\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8664 - val_loss: 38.9269\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7344 - val_loss: 38.8468\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.7960 - val_loss: 39.2990\n",
      "Epoch 00031: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 44.9220 - val_loss: 40.6219\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.3030 - val_loss: 40.2548\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6855 - val_loss: 39.5037\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4471 - val_loss: 39.7456\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3171 - val_loss: 39.3001\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2109 - val_loss: 39.1625\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1469 - val_loss: 39.2558\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0102 - val_loss: 39.0440\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0759 - val_loss: 38.8438\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9533 - val_loss: 38.7626\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7655 - val_loss: 38.7294\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8591 - val_loss: 38.9494\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8690 - val_loss: 38.7145\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7617 - val_loss: 38.7191\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7530 - val_loss: 38.6780\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7141 - val_loss: 38.4786\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6820 - val_loss: 38.9463\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6682 - val_loss: 38.5724\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6475 - val_loss: 39.0366\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6524 - val_loss: 38.7970\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.6009 - val_loss: 38.8872\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5427 - val_loss: 38.6844\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6129 - val_loss: 39.1274\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5857 - val_loss: 38.7102\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6748 - val_loss: 38.9655\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5994 - val_loss: 38.8506\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.9437 - val_loss: 40.5448\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 40.0501 - val_loss: 39.4197\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5932 - val_loss: 39.1869\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3308 - val_loss: 38.8354\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3090 - val_loss: 39.2686\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1276 - val_loss: 38.5978\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0460 - val_loss: 39.1350\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9965 - val_loss: 39.1052\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0027 - val_loss: 38.5605\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9567 - val_loss: 38.5788\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8890 - val_loss: 39.0619\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8905 - val_loss: 38.7182\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7897 - val_loss: 38.9625\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8194 - val_loss: 38.6651\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8263 - val_loss: 39.0694\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8064 - val_loss: 38.6809\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8288 - val_loss: 38.7818\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7888 - val_loss: 39.1330\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8343 - val_loss: 38.9114\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 43.7316 - val_loss: 40.2107\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8198 - val_loss: 39.2199\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4673 - val_loss: 39.2640\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1380 - val_loss: 39.2016\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0554 - val_loss: 39.2795\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9541 - val_loss: 39.2348\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8866 - val_loss: 38.6677\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8462 - val_loss: 38.6981\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7701 - val_loss: 38.9380\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8165 - val_loss: 38.9761\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7655 - val_loss: 38.8124\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7005 - val_loss: 38.7368\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7657 - val_loss: 38.5592\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7076 - val_loss: 38.7278\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6179 - val_loss: 39.4876\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6998 - val_loss: 38.8396\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6887 - val_loss: 38.6353\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6440 - val_loss: 38.7461\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6564 - val_loss: 39.0531\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5690 - val_loss: 38.8564\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6638 - val_loss: 38.8974\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6007 - val_loss: 38.8894\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5747 - val_loss: 39.1010\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 44.0770 - val_loss: 40.8942\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9409 - val_loss: 40.1233\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5706 - val_loss: 39.5760\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2931 - val_loss: 39.0952\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1456 - val_loss: 39.3585\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0706 - val_loss: 38.9612\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9336 - val_loss: 38.9633\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8499 - val_loss: 39.0720\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7110 - val_loss: 38.9141\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7836 - val_loss: 38.7868\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6799 - val_loss: 38.7972\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6160 - val_loss: 39.0088\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6630 - val_loss: 38.6032\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6125 - val_loss: 38.7959\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5793 - val_loss: 38.9599\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6174 - val_loss: 38.4380\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6196 - val_loss: 38.8647\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6314 - val_loss: 38.5063\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5283 - val_loss: 38.7743\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6096 - val_loss: 38.6945\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5698 - val_loss: 38.8022\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5375 - val_loss: 38.8385\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5082 - val_loss: 38.6716\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4547 - val_loss: 38.9942\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4964 - val_loss: 38.9724\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5155 - val_loss: 38.7190\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 43.6530 - val_loss: 40.1493\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7631 - val_loss: 39.8153\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3536 - val_loss: 39.2401\n",
      "Epoch 4/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1379 - val_loss: 39.1069\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9894 - val_loss: 38.7997\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9533 - val_loss: 38.9272\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9019 - val_loss: 38.6328\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8382 - val_loss: 38.7873\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7834 - val_loss: 38.6052\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7961 - val_loss: 39.2287\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7009 - val_loss: 39.0597\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7552 - val_loss: 39.1624\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6462 - val_loss: 38.8987\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5587 - val_loss: 39.1109\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6033 - val_loss: 38.7179\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6008 - val_loss: 38.6280\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5596 - val_loss: 38.6249\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6117 - val_loss: 38.7841\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5397 - val_loss: 38.9831\n",
      "Epoch 00019: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 49.0895 - val_loss: 41.5599\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 42.8789 - val_loss: 40.0214\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.8481 - val_loss: 39.7386\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.3560 - val_loss: 39.8140\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.1242 - val_loss: 39.9573\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.8181 - val_loss: 39.5336\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.5724 - val_loss: 39.4830\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.5987 - val_loss: 39.7308\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.4715 - val_loss: 39.3856\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.3657 - val_loss: 39.7499\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.2568 - val_loss: 39.5489\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.1705 - val_loss: 39.4944\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.2316 - val_loss: 39.6950\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9830 - val_loss: 39.5131\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8465 - val_loss: 39.6421\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8599 - val_loss: 39.1720\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8436 - val_loss: 39.5047\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7472 - val_loss: 39.4860\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7473 - val_loss: 39.3854\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6617 - val_loss: 39.3431\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7998 - val_loss: 39.4858\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7218 - val_loss: 39.5361\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4625 - val_loss: 39.5338\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5618 - val_loss: 39.3884\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5307 - val_loss: 39.3769\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4555 - val_loss: 39.2983\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 48.6864 - val_loss: 41.2034\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.6418 - val_loss: 39.8580\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.8148 - val_loss: 40.1172\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.4128 - val_loss: 39.3788\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.3874 - val_loss: 39.3008\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.0208 - val_loss: 39.1726\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.0854 - val_loss: 39.4351\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8226 - val_loss: 38.9408\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7938 - val_loss: 39.1751\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7334 - val_loss: 38.9893\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6947 - val_loss: 39.3527\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5048 - val_loss: 39.3488\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5757 - val_loss: 39.2123\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4700 - val_loss: 39.2285\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4271 - val_loss: 39.0982\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2960 - val_loss: 39.1559\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3471 - val_loss: 38.9923\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2106 - val_loss: 39.2024\n",
      "Epoch 00018: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 46.7221 - val_loss: 42.1216\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.9545 - val_loss: 40.0349\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.2302 - val_loss: 39.8058\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8680 - val_loss: 39.7988\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6596 - val_loss: 39.4422\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5897 - val_loss: 39.1176\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4601 - val_loss: 39.5430\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4027 - val_loss: 39.3093\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3982 - val_loss: 39.0128\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.3107 - val_loss: 39.0540\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2472 - val_loss: 39.0875\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2903 - val_loss: 38.9862\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1940 - val_loss: 39.1724\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0976 - val_loss: 39.1773\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0447 - val_loss: 39.2718\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0775 - val_loss: 39.0938\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0664 - val_loss: 39.0582\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0440 - val_loss: 39.0907\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0674 - val_loss: 39.3647\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9374 - val_loss: 38.9429\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.9705 - val_loss: 38.9678\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9017 - val_loss: 39.1719\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8895 - val_loss: 38.6760\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9850 - val_loss: 38.7759\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9658 - val_loss: 39.2074\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9589 - val_loss: 38.7605\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8424 - val_loss: 38.8244\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7884 - val_loss: 38.7836\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8636 - val_loss: 39.2278\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7856 - val_loss: 39.0856\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7776 - val_loss: 38.8396\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7807 - val_loss: 38.7326\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7639 - val_loss: 39.0210\n",
      "Epoch 00033: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 46.2750 - val_loss: 40.3177\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.5888 - val_loss: 39.9426\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9493 - val_loss: 39.6736\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7899 - val_loss: 39.3535\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 39.5575 - val_loss: 39.4041\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4657 - val_loss: 38.7755\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3563 - val_loss: 39.2830\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3224 - val_loss: 39.1008\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1963 - val_loss: 38.9105\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1248 - val_loss: 39.2062\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0546 - val_loss: 39.2154\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1299 - val_loss: 38.9694\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0603 - val_loss: 39.6930\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0165 - val_loss: 39.0643\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9569 - val_loss: 39.0756\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9059 - val_loss: 39.0290\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 44.9663 - val_loss: 40.3950\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1676 - val_loss: 39.7941\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6617 - val_loss: 39.2557\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4503 - val_loss: 39.5088\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3712 - val_loss: 39.5294\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2250 - val_loss: 39.5394\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0402 - val_loss: 39.0775\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1078 - val_loss: 39.3933\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9221 - val_loss: 39.0959\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9154 - val_loss: 39.1017\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8404 - val_loss: 38.9709\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7799 - val_loss: 39.0604\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8164 - val_loss: 38.9507\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7485 - val_loss: 38.9107\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7842 - val_loss: 39.1582\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7874 - val_loss: 38.8341\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6993 - val_loss: 38.8598\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6680 - val_loss: 38.9608\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7170 - val_loss: 39.1685\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6650 - val_loss: 38.9199\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6936 - val_loss: 38.8509\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6834 - val_loss: 39.0235\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7089 - val_loss: 39.2771\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6676 - val_loss: 39.1375\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5850 - val_loss: 38.7115\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6410 - val_loss: 39.2595\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5722 - val_loss: 38.8196\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6536 - val_loss: 38.9149\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5899 - val_loss: 39.0947\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6046 - val_loss: 39.2334\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6042 - val_loss: 39.1780\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5933 - val_loss: 39.0020\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5924 - val_loss: 39.1076\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5633 - val_loss: 39.1828\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4119 - val_loss: 39.0628\n",
      "Epoch 00035: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.5449 - val_loss: 41.4400\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.4693 - val_loss: 40.5801\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8768 - val_loss: 39.6797\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4618 - val_loss: 39.0709\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3245 - val_loss: 39.2294\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1793 - val_loss: 39.2645\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1717 - val_loss: 39.6830\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9934 - val_loss: 38.8775\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9882 - val_loss: 38.9184\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9685 - val_loss: 38.8027\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9576 - val_loss: 38.7868\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9327 - val_loss: 38.9562\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8733 - val_loss: 38.6955\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7676 - val_loss: 39.2172\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8433 - val_loss: 39.0976\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7948 - val_loss: 39.0446\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7711 - val_loss: 39.1594\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6740 - val_loss: 38.8728\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7279 - val_loss: 39.5769\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7139 - val_loss: 38.8086\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6778 - val_loss: 39.2473\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6915 - val_loss: 38.8873\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6136 - val_loss: 39.0634\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 46.8145 - val_loss: 40.5621\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.6378 - val_loss: 40.0321\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9098 - val_loss: 39.4591\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4352 - val_loss: 39.5267\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3157 - val_loss: 39.7191\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1971 - val_loss: 38.9325\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9772 - val_loss: 38.7947\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9942 - val_loss: 39.0920\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8401 - val_loss: 39.0941\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8611 - val_loss: 39.2357\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8075 - val_loss: 40.0014\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8669 - val_loss: 38.8305\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8619 - val_loss: 38.8281\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7669 - val_loss: 38.9929\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7666 - val_loss: 39.0326\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7134 - val_loss: 38.8405\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7607 - val_loss: 38.9167\n",
      "Epoch 00017: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.0955 - val_loss: 40.4663\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0065 - val_loss: 39.7350\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4359 - val_loss: 39.4647\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3142 - val_loss: 39.1355\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1840 - val_loss: 39.3436\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0339 - val_loss: 39.3148\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9080 - val_loss: 38.8820\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8548 - val_loss: 38.9208\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9021 - val_loss: 39.1031\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7349 - val_loss: 38.8971\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7909 - val_loss: 38.8583\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7457 - val_loss: 38.7897\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7665 - val_loss: 38.8139\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6813 - val_loss: 38.6969\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6657 - val_loss: 38.9762\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6341 - val_loss: 38.7055\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5982 - val_loss: 38.9916\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6772 - val_loss: 38.8272\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5465 - val_loss: 38.6206\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5998 - val_loss: 38.9743\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6434 - val_loss: 39.2468\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5421 - val_loss: 39.1453\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4889 - val_loss: 38.6283\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5337 - val_loss: 38.8348\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5414 - val_loss: 38.7212\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5651 - val_loss: 39.0775\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5295 - val_loss: 38.5557\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5148 - val_loss: 38.6453\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4739 - val_loss: 38.9299\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4761 - val_loss: 38.5241\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4902 - val_loss: 38.4912\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5022 - val_loss: 38.8708\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5370 - val_loss: 38.7433\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4633 - val_loss: 38.5495\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5058 - val_loss: 38.9946\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4421 - val_loss: 38.6549\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4334 - val_loss: 38.6479\n",
      "Epoch 38/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5015 - val_loss: 38.8344\n",
      "Epoch 39/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4346 - val_loss: 38.6594\n",
      "Epoch 40/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4195 - val_loss: 38.5905\n",
      "Epoch 41/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4187 - val_loss: 38.6396\n",
      "Epoch 00041: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 48.8882 - val_loss: 41.6610\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 42.4235 - val_loss: 40.1623\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 41.3756 - val_loss: 39.9169\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.7518 - val_loss: 39.8892\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.5934 - val_loss: 39.8535\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.4630 - val_loss: 39.7612\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.5211 - val_loss: 39.7474\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.5015 - val_loss: 39.7894\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.2243 - val_loss: 39.7371\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.0593 - val_loss: 39.7339\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9401 - val_loss: 39.6779\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 40.0004 - val_loss: 39.6178\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.9326 - val_loss: 39.6918\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8739 - val_loss: 39.6472\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8893 - val_loss: 39.8126\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7526 - val_loss: 39.6493\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8779 - val_loss: 39.7281\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.7505 - val_loss: 39.5870\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6095 - val_loss: 39.6095\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6711 - val_loss: 39.8072\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.6112 - val_loss: 39.6014\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5348 - val_loss: 39.6954\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4297 - val_loss: 39.6506\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4562 - val_loss: 39.7249\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4398 - val_loss: 39.6090\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.2937 - val_loss: 39.7580\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4116 - val_loss: 39.6354\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.4045 - val_loss: 39.6688\n",
      "Epoch 00028: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 46.5790 - val_loss: 41.1413\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 41.2997 - val_loss: 39.6881\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.6037 - val_loss: 39.6007\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.3319 - val_loss: 39.5898\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0462 - val_loss: 39.5453\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8381 - val_loss: 39.6988\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9116 - val_loss: 39.8616\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7820 - val_loss: 39.8161\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8174 - val_loss: 39.4254\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7259 - val_loss: 39.3261\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6833 - val_loss: 39.4007\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5915 - val_loss: 39.5643\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.5168 - val_loss: 39.4591\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3418 - val_loss: 39.4294\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2558 - val_loss: 39.4293\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3845 - val_loss: 39.3116\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2913 - val_loss: 39.5031\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3042 - val_loss: 39.3904\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2480 - val_loss: 39.5038\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2038 - val_loss: 39.5378\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1624 - val_loss: 39.4086\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2637 - val_loss: 39.5532\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1650 - val_loss: 39.6018\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1057 - val_loss: 38.9134\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1075 - val_loss: 39.0944\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.1152 - val_loss: 39.3134\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 38.8826 - val_loss: 39.1891\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0930 - val_loss: 39.2994\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0157 - val_loss: 39.2102\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.9446 - val_loss: 39.2890\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0049 - val_loss: 39.4843\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0131 - val_loss: 39.4866\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9338 - val_loss: 39.1888\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0155 - val_loss: 39.4914\n",
      "Epoch 00034: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.7933 - val_loss: 39.9827\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.6452 - val_loss: 39.5815\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9497 - val_loss: 39.3179\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8244 - val_loss: 39.2970\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6585 - val_loss: 39.1938\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5965 - val_loss: 39.2275\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5506 - val_loss: 38.9820\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4684 - val_loss: 39.0967\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.4592 - val_loss: 39.2239\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2586 - val_loss: 39.4332\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2527 - val_loss: 39.0323\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2807 - val_loss: 38.9102\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1864 - val_loss: 38.8161\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1487 - val_loss: 39.0208\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2318 - val_loss: 39.0801\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1708 - val_loss: 39.0982\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0960 - val_loss: 39.3853\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0182 - val_loss: 38.9578\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9740 - val_loss: 39.4721\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0451 - val_loss: 38.7361\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0112 - val_loss: 38.9417\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9342 - val_loss: 39.2161\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0205 - val_loss: 39.2284\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9261 - val_loss: 38.9776\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9601 - val_loss: 39.0028\n",
      "Epoch 26/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7886 - val_loss: 39.2558\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8140 - val_loss: 39.0782\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8159 - val_loss: 39.1306\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8022 - val_loss: 38.7153\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7732 - val_loss: 39.0410\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6940 - val_loss: 39.0750\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7897 - val_loss: 39.0178\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7256 - val_loss: 39.1646\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6109 - val_loss: 39.2687\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8014 - val_loss: 39.2561\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7766 - val_loss: 39.0205\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7579 - val_loss: 39.3628\n",
      "Epoch 38/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8191 - val_loss: 39.3808\n",
      "Epoch 39/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7012 - val_loss: 39.1490\n",
      "Epoch 00039: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.7266 - val_loss: 40.6055\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.3636 - val_loss: 39.6018\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8331 - val_loss: 39.4734\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4497 - val_loss: 39.9028\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3923 - val_loss: 39.7244\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2059 - val_loss: 39.5598\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1599 - val_loss: 39.3027\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1070 - val_loss: 39.7047\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1932 - val_loss: 38.9588\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9896 - val_loss: 38.9363\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9628 - val_loss: 39.0868\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0459 - val_loss: 38.9979\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9692 - val_loss: 39.0779\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8883 - val_loss: 39.0433\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9349 - val_loss: 39.0246\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8622 - val_loss: 38.9541\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8020 - val_loss: 38.8552\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8606 - val_loss: 39.3125\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8127 - val_loss: 39.0744\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7546 - val_loss: 38.8763\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8105 - val_loss: 39.2041\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7091 - val_loss: 38.9748\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7848 - val_loss: 38.8267\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6637 - val_loss: 38.9368\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6766 - val_loss: 38.9617\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6301 - val_loss: 38.8461\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6622 - val_loss: 39.2356\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6673 - val_loss: 38.8658\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6505 - val_loss: 38.8462\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6301 - val_loss: 38.7212\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6063 - val_loss: 39.3048\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5971 - val_loss: 39.2837\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7062 - val_loss: 38.8441\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6004 - val_loss: 39.1934\n",
      "Epoch 35/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6674 - val_loss: 38.8602\n",
      "Epoch 36/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5374 - val_loss: 38.8888\n",
      "Epoch 37/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5993 - val_loss: 39.1496\n",
      "Epoch 38/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5710 - val_loss: 39.2759\n",
      "Epoch 39/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5570 - val_loss: 39.4533\n",
      "Epoch 40/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5651 - val_loss: 39.1248\n",
      "Epoch 00040: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 43.3751 - val_loss: 39.6301\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9294 - val_loss: 39.2798\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6381 - val_loss: 38.8517\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3157 - val_loss: 39.1594\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1097 - val_loss: 38.9638\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0472 - val_loss: 38.7854\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0282 - val_loss: 38.7030\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0125 - val_loss: 39.1139\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9831 - val_loss: 39.1870\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9138 - val_loss: 38.8517\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9272 - val_loss: 39.1800\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8565 - val_loss: 38.7695\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8179 - val_loss: 38.9751\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8416 - val_loss: 38.8627\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7883 - val_loss: 39.5808\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7662 - val_loss: 38.8210\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6947 - val_loss: 39.3412\n",
      "Epoch 00017: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 44.3894 - val_loss: 39.7394\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9290 - val_loss: 39.4785\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5618 - val_loss: 39.4210\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2127 - val_loss: 38.9120\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1163 - val_loss: 38.9967\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1096 - val_loss: 38.8779\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0136 - val_loss: 38.7171\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9444 - val_loss: 39.1604\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8682 - val_loss: 38.7755\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7542 - val_loss: 38.6050\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8087 - val_loss: 38.5458\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8339 - val_loss: 39.0516\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8366 - val_loss: 38.5830\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7968 - val_loss: 39.2565\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7551 - val_loss: 38.7640\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7315 - val_loss: 39.4177\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7012 - val_loss: 39.0621\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7407 - val_loss: 38.4106\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6947 - val_loss: 38.6346\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7356 - val_loss: 38.4980\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7717 - val_loss: 38.7867\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6029 - val_loss: 38.8065\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6823 - val_loss: 38.8152\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6471 - val_loss: 38.8539\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6367 - val_loss: 38.5472\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6515 - val_loss: 38.6600\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6277 - val_loss: 38.8476\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6125 - val_loss: 38.9544\n",
      "Epoch 00028: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 47.2161 - val_loss: 40.3065\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9723 - val_loss: 39.2270\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.5824 - val_loss: 39.1501\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2958 - val_loss: 38.6286\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1180 - val_loss: 39.0828\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9814 - val_loss: 39.3508\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9856 - val_loss: 38.8862\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9083 - val_loss: 39.0092\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7940 - val_loss: 39.3396\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9670 - val_loss: 38.9956\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6814 - val_loss: 38.9399\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7697 - val_loss: 38.8132\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7109 - val_loss: 38.5499\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6398 - val_loss: 38.7027\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6650 - val_loss: 38.6623\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6969 - val_loss: 39.3372\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6350 - val_loss: 38.6661\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6467 - val_loss: 39.0747\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5990 - val_loss: 38.9170\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6021 - val_loss: 38.6229\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5459 - val_loss: 38.7840\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5570 - val_loss: 38.6426\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6132 - val_loss: 38.8241\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 45.7313 - val_loss: 40.3471\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.8575 - val_loss: 39.7557\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5132 - val_loss: 39.2357\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2023 - val_loss: 39.5374\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1749 - val_loss: 38.9315\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0948 - val_loss: 39.4145\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.9877 - val_loss: 39.3937\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0500 - val_loss: 39.3266\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8831 - val_loss: 39.3293\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8117 - val_loss: 39.2769\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8624 - val_loss: 38.8045\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7751 - val_loss: 38.9157\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7409 - val_loss: 38.9062\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7327 - val_loss: 39.3651\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8170 - val_loss: 39.1547\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7331 - val_loss: 38.8219\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7238 - val_loss: 39.0524\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.6844 - val_loss: 39.0897\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.6901 - val_loss: 39.1451\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6181 - val_loss: 38.9144\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6277 - val_loss: 38.9914\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 49.2530 - val_loss: 42.5922\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 42.5094 - val_loss: 40.2535\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 41.3205 - val_loss: 40.0216\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.9018 - val_loss: 39.9100\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.7745 - val_loss: 39.8078\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.6520 - val_loss: 39.9850\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.5088 - val_loss: 39.7845\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.3044 - val_loss: 39.7473\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1979 - val_loss: 39.7710\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0271 - val_loss: 39.8185\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1170 - val_loss: 39.8235\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0290 - val_loss: 39.7141\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1067 - val_loss: 39.8565\n",
      "Epoch 14/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0175 - val_loss: 39.8571\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8668 - val_loss: 39.6629\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8341 - val_loss: 39.8131\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 1ms/step - loss: 39.8129 - val_loss: 39.6531\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7980 - val_loss: 39.8247\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6101 - val_loss: 39.7807\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6971 - val_loss: 39.7690\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6033 - val_loss: 39.7453\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4978 - val_loss: 39.6773\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5523 - val_loss: 39.8051\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5113 - val_loss: 39.7541\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4038 - val_loss: 39.7457\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4193 - val_loss: 39.7023\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5142 - val_loss: 39.7660\n",
      "Epoch 00027: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 46.8585 - val_loss: 40.4330\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 41.1338 - val_loss: 39.7695\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.2280 - val_loss: 39.5763\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1736 - val_loss: 39.5523\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7466 - val_loss: 39.4362\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8701 - val_loss: 39.4586\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7112 - val_loss: 39.3910\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6937 - val_loss: 39.5131\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5654 - val_loss: 39.5804\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4915 - val_loss: 39.4713\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4616 - val_loss: 39.3660\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4437 - val_loss: 39.4749\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4868 - val_loss: 39.6025\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5755 - val_loss: 39.3774\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3287 - val_loss: 39.4884\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4484 - val_loss: 39.5891\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2675 - val_loss: 39.4337\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3574 - val_loss: 39.6107\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2136 - val_loss: 39.5543\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1584 - val_loss: 39.4155\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2243 - val_loss: 39.3826\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.7564 - val_loss: 40.8899\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.9389 - val_loss: 39.9142\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1419 - val_loss: 39.9236\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8628 - val_loss: 39.3753\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6471 - val_loss: 39.4091\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5611 - val_loss: 39.4197\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4803 - val_loss: 39.0183\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3492 - val_loss: 39.2424\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2913 - val_loss: 38.9620\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1305 - val_loss: 38.9957\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2439 - val_loss: 38.8806\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1308 - val_loss: 39.3046\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0619 - val_loss: 39.0846\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0174 - val_loss: 39.0196\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9649 - val_loss: 38.7724\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9572 - val_loss: 39.2344\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9573 - val_loss: 38.8385\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8098 - val_loss: 38.9989\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8754 - val_loss: 39.1102\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8757 - val_loss: 38.8954\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8522 - val_loss: 39.0005\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8297 - val_loss: 39.2790\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7886 - val_loss: 38.8420\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7775 - val_loss: 38.7787\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7468 - val_loss: 38.8365\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.2568 - val_loss: 40.3900\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.5350 - val_loss: 40.2772\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0989 - val_loss: 39.4436\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6980 - val_loss: 39.3383\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6823 - val_loss: 39.4493\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4327 - val_loss: 38.9746\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3223 - val_loss: 39.0133\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2367 - val_loss: 38.8258\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2064 - val_loss: 38.9089\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1644 - val_loss: 39.2654\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0638 - val_loss: 39.0969\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1608 - val_loss: 38.8788\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0594 - val_loss: 39.1692\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0984 - val_loss: 38.9720\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0674 - val_loss: 38.7005\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9793 - val_loss: 39.1536\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0283 - val_loss: 38.4970\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9375 - val_loss: 39.1430\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9783 - val_loss: 38.8437\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8455 - val_loss: 39.1383\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0359 - val_loss: 38.9765\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9317 - val_loss: 38.8714\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8939 - val_loss: 39.1273\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8152 - val_loss: 39.0596\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9411 - val_loss: 39.1284\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8208 - val_loss: 39.0497\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8707 - val_loss: 38.7741\n",
      "Epoch 00027: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 45.0360 - val_loss: 40.6390\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1144 - val_loss: 39.6627\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6488 - val_loss: 39.5844\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5222 - val_loss: 39.4988\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2703 - val_loss: 39.6990\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2068 - val_loss: 39.2145\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0185 - val_loss: 39.3165\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0407 - val_loss: 38.9564\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0985 - val_loss: 39.4693\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9118 - val_loss: 39.2120\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8334 - val_loss: 39.1251\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8237 - val_loss: 39.0849\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8555 - val_loss: 39.6101\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8577 - val_loss: 39.3396\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7869 - val_loss: 39.1774\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8997 - val_loss: 38.9752\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7616 - val_loss: 38.9423\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7331 - val_loss: 38.9138\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7232 - val_loss: 39.2464\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6964 - val_loss: 39.0692\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7071 - val_loss: 38.7817\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6643 - val_loss: 38.6850\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6450 - val_loss: 39.0808\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7084 - val_loss: 39.2230\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6871 - val_loss: 39.1537\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6986 - val_loss: 39.1870\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7242 - val_loss: 39.1245\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6730 - val_loss: 39.0515\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5872 - val_loss: 38.9897\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6232 - val_loss: 39.0493\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5203 - val_loss: 39.5525\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5960 - val_loss: 39.3594\n",
      "Epoch 00032: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 44.2829 - val_loss: 40.7823\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0528 - val_loss: 39.8389\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4978 - val_loss: 39.3092\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3484 - val_loss: 39.5927\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1048 - val_loss: 39.1367\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0396 - val_loss: 39.5243\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0436 - val_loss: 39.0632\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9859 - val_loss: 39.2403\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9310 - val_loss: 39.3886\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7744 - val_loss: 39.5843\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9111 - val_loss: 38.9703\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7872 - val_loss: 39.1685\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7572 - val_loss: 39.0512\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7157 - val_loss: 39.4195\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7714 - val_loss: 39.8266\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6941 - val_loss: 39.0734\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6217 - val_loss: 38.9486\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6764 - val_loss: 39.2628\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5954 - val_loss: 39.2438\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6129 - val_loss: 39.2329\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5745 - val_loss: 38.8003\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6362 - val_loss: 39.4259\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6266 - val_loss: 38.9767\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5759 - val_loss: 39.2341\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5579 - val_loss: 39.0924\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6020 - val_loss: 39.5046\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6673 - val_loss: 39.6128\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5779 - val_loss: 39.2034\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5355 - val_loss: 39.3049\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.5120 - val_loss: 39.2707\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.4796 - val_loss: 39.0258\n",
      "Epoch 00031: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 44.4175 - val_loss: 41.5779\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 40.0307 - val_loss: 40.4656\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6448 - val_loss: 39.7507\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.4110 - val_loss: 39.6378\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.2053 - val_loss: 39.3114\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.1765 - val_loss: 39.6861\n",
      "Epoch 7/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0040 - val_loss: 39.3757\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0906 - val_loss: 39.5115\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.9621 - val_loss: 39.0267\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7775 - val_loss: 39.2450\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8690 - val_loss: 39.1712\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7742 - val_loss: 38.9242\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7612 - val_loss: 38.6857\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.8047 - val_loss: 39.0468\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.7582 - val_loss: 39.2387\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.7487 - val_loss: 38.6924\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.8041 - val_loss: 39.2603\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.6800 - val_loss: 38.8386\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.7471 - val_loss: 38.9079\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.7710 - val_loss: 38.7790\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.6391 - val_loss: 38.7399\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.6987 - val_loss: 39.7179\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.7346 - val_loss: 39.0093\n",
      "Epoch 00023: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 45.2293 - val_loss: 42.0224\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 40.2815 - val_loss: 39.6440\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.5674 - val_loss: 39.6122\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.4101 - val_loss: 39.3419\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.2568 - val_loss: 38.9236\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.1002 - val_loss: 39.7908\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0367 - val_loss: 38.8465\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0434 - val_loss: 39.1772\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0034 - val_loss: 39.4866\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0065 - val_loss: 39.4613\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0019 - val_loss: 38.9726\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.9843 - val_loss: 38.6180\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.9339 - val_loss: 39.1027\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.8288 - val_loss: 39.1406\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.7940 - val_loss: 39.0961\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.8331 - val_loss: 39.0349\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.7827 - val_loss: 39.0425\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.8470 - val_loss: 39.4016\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.6395 - val_loss: 38.6716\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.7849 - val_loss: 38.8538\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.7149 - val_loss: 39.3081\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.6957 - val_loss: 38.8555\n",
      "Epoch 00022: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 48.9633 - val_loss: 41.1468\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 42.4720 - val_loss: 40.4396\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 41.5473 - val_loss: 40.0464\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 41.0587 - val_loss: 39.9767\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.8584 - val_loss: 39.9439\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.7946 - val_loss: 39.9159\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.5352 - val_loss: 39.8955\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.4742 - val_loss: 39.9251\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.5105 - val_loss: 39.8410\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.4025 - val_loss: 39.9266\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.3117 - val_loss: 40.0212\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.2425 - val_loss: 39.8946\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.2150 - val_loss: 39.9230\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1566 - val_loss: 39.8127\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1833 - val_loss: 39.8521\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0466 - val_loss: 39.8200\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9957 - val_loss: 39.8129\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9744 - val_loss: 39.8841\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8084 - val_loss: 39.8020\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7971 - val_loss: 39.8025\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7933 - val_loss: 39.6987\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6648 - val_loss: 39.7729\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6412 - val_loss: 39.7977\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5624 - val_loss: 39.6304\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5893 - val_loss: 39.7201\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5582 - val_loss: 39.7211\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5615 - val_loss: 39.7052\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4430 - val_loss: 39.6614\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4560 - val_loss: 39.7110\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4748 - val_loss: 39.6735\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3781 - val_loss: 39.7216\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5616 - val_loss: 39.6529\n",
      "Epoch 33/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4133 - val_loss: 39.7518\n",
      "Epoch 34/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3715 - val_loss: 39.6821\n",
      "Epoch 00034: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 47.6343 - val_loss: 40.7020\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 41.4320 - val_loss: 40.0643\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.6035 - val_loss: 39.7390\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.2915 - val_loss: 39.6348\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.3142 - val_loss: 39.8021\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0582 - val_loss: 39.8570\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.9987 - val_loss: 39.5638\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8626 - val_loss: 39.5767\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7390 - val_loss: 39.6372\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7602 - val_loss: 39.3146\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7420 - val_loss: 39.6094\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7050 - val_loss: 39.5045\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6067 - val_loss: 39.2796\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4965 - val_loss: 39.3332\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4621 - val_loss: 39.5640\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5060 - val_loss: 39.3561\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4049 - val_loss: 39.2835\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4581 - val_loss: 39.6078\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3614 - val_loss: 39.5611\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4537 - val_loss: 39.6375\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3197 - val_loss: 39.3967\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2966 - val_loss: 39.1009\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1546 - val_loss: 39.4631\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2078 - val_loss: 39.2439\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2010 - val_loss: 39.4692\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1217 - val_loss: 39.3092\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9651 - val_loss: 39.1486\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0487 - val_loss: 39.4323\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0582 - val_loss: 39.4380\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1083 - val_loss: 39.6908\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0284 - val_loss: 39.3970\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0266 - val_loss: 39.2833\n",
      "Epoch 00032: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 46.6872 - val_loss: 40.1255\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.9836 - val_loss: 39.7619\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.3319 - val_loss: 39.7217\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1906 - val_loss: 39.7308\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8495 - val_loss: 39.6161\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.8270 - val_loss: 39.4078\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.7571 - val_loss: 39.6187\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5489 - val_loss: 39.5585\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5672 - val_loss: 39.3310\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4543 - val_loss: 39.3113\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4837 - val_loss: 39.2486\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3468 - val_loss: 39.3392\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2276 - val_loss: 39.6612\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2024 - val_loss: 39.4151\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2287 - val_loss: 39.6582\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2480 - val_loss: 39.2869\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3128 - val_loss: 39.5951\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0924 - val_loss: 39.6272\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1877 - val_loss: 39.4939\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1140 - val_loss: 39.6505\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9279 - val_loss: 39.4904\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 46.6059 - val_loss: 40.7035\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 41.2482 - val_loss: 39.9841\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.1554 - val_loss: 40.4189\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 40.0323 - val_loss: 39.7756\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.6642 - val_loss: 38.9707\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5826 - val_loss: 39.5057\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5946 - val_loss: 39.1905\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5723 - val_loss: 39.3826\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4270 - val_loss: 39.4602\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3933 - val_loss: 38.9469\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4470 - val_loss: 39.0854\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.3151 - val_loss: 38.8646\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2499 - val_loss: 38.9794\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2503 - val_loss: 39.0728\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1690 - val_loss: 39.1156\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1379 - val_loss: 39.1975\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1632 - val_loss: 38.8250\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1231 - val_loss: 39.0269\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.1744 - val_loss: 39.0420\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9755 - val_loss: 39.0836\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0534 - val_loss: 39.2519\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.9393 - val_loss: 38.7978\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0994 - val_loss: 39.2315\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9800 - val_loss: 39.0050\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.9380 - val_loss: 39.2283\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8822 - val_loss: 39.0263\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8950 - val_loss: 38.9755\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9824 - val_loss: 39.0973\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8637 - val_loss: 38.9759\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9195 - val_loss: 39.0736\n",
      "Epoch 31/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8581 - val_loss: 39.3196\n",
      "Epoch 32/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9457 - val_loss: 39.1200\n",
      "Epoch 00032: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 46.3854 - val_loss: 40.5313\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 40.7214 - val_loss: 39.7684\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.8694 - val_loss: 39.8977\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.5792 - val_loss: 39.4587\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4625 - val_loss: 39.0271\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2266 - val_loss: 39.2579\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.2803 - val_loss: 39.3107\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0840 - val_loss: 39.1211\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0465 - val_loss: 39.1397\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.1209 - val_loss: 38.9381\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9789 - val_loss: 38.6122\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.0184 - val_loss: 39.1024\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9956 - val_loss: 39.0983\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8640 - val_loss: 39.1723\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.9041 - val_loss: 38.9371\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8688 - val_loss: 39.0442\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9087 - val_loss: 38.6950\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8810 - val_loss: 38.8886\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9690 - val_loss: 38.9447\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8393 - val_loss: 39.1680\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.8794 - val_loss: 38.9037\n",
      "Epoch 00021: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 45.3533 - val_loss: 40.5986\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 40.2998 - val_loss: 39.9316\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.7270 - val_loss: 39.6450\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 39.4252 - val_loss: 39.2816\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 24s 98ms/step - loss: 39.2745 - val_loss: 38.6954\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 39.0858 - val_loss: 38.7334\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 39.0873 - val_loss: 38.7367\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.9998 - val_loss: 38.6561\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9978 - val_loss: 39.0445\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 0s 2ms/step - loss: 38.9739 - val_loss: 39.0407\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.9301 - val_loss: 38.8284\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 6s 25ms/step - loss: 38.8540 - val_loss: 38.7947\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 6s 26ms/step - loss: 38.8722 - val_loss: 38.8619\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 1979s 8s/step - loss: 38.7693 - val_loss: 38.8001\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.7966 - val_loss: 38.7630\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 38.8135 - val_loss: 38.5856\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.7394 - val_loss: 38.9345\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.6833 - val_loss: 38.6829\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.7507 - val_loss: 39.1862\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.7238 - val_loss: 38.6282\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 38.6757 - val_loss: 38.7263\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 38.7425 - val_loss: 39.1067\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.7067 - val_loss: 38.7800\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.7328 - val_loss: 38.8609\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.6725 - val_loss: 39.0239\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 38.5535 - val_loss: 38.6331\n",
      "Epoch 00026: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 8s 33ms/step - loss: 44.9650 - val_loss: 40.8133\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 7s 29ms/step - loss: 39.8404 - val_loss: 40.1726\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 7s 27ms/step - loss: 39.4533 - val_loss: 39.6800\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 6s 25ms/step - loss: 39.1808 - val_loss: 38.8844\n",
      "Epoch 5/150\n",
      "248/248 [==============================] - 7s 26ms/step - loss: 39.1413 - val_loss: 39.1945\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 6s 24ms/step - loss: 38.9022 - val_loss: 39.0867\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 5s 20ms/step - loss: 38.9257 - val_loss: 38.8794\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 38.9723 - val_loss: 39.3488\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 2s 10ms/step - loss: 38.7943 - val_loss: 39.5781\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 38.7890 - val_loss: 39.2072\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 2s 7ms/step - loss: 38.8503 - val_loss: 38.9429\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 38.7358 - val_loss: 38.6687\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 38.6873 - val_loss: 38.9775\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.7128 - val_loss: 38.8420\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6615 - val_loss: 38.5588\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 38.6540 - val_loss: 39.3628\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6724 - val_loss: 38.7317\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6088 - val_loss: 38.9232\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.5463 - val_loss: 38.7662\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6125 - val_loss: 38.7669\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6098 - val_loss: 39.0538\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.5454 - val_loss: 39.0504\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.5663 - val_loss: 39.2647\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6084 - val_loss: 38.8459\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.5550 - val_loss: 38.6997\n",
      "Epoch 00025: early stopping\n",
      "Epoch 1/150\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 44.2004 - val_loss: 40.9198\n",
      "Epoch 2/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 39.8323 - val_loss: 40.0781\n",
      "Epoch 3/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 39.4101 - val_loss: 39.6547\n",
      "Epoch 4/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 39.3708 - val_loss: 39.7834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/150\n",
      "248/248 [==============================] - 2s 6ms/step - loss: 39.0587 - val_loss: 39.6818\n",
      "Epoch 6/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 39.1229 - val_loss: 39.6580\n",
      "Epoch 7/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 38.9484 - val_loss: 39.6214\n",
      "Epoch 8/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 38.9272 - val_loss: 38.7849\n",
      "Epoch 9/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 38.9412 - val_loss: 39.1907\n",
      "Epoch 10/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 38.8890 - val_loss: 39.4778\n",
      "Epoch 11/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 38.8211 - val_loss: 39.3104\n",
      "Epoch 12/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 38.7346 - val_loss: 38.9018\n",
      "Epoch 13/150\n",
      "248/248 [==============================] - 1s 6ms/step - loss: 38.7414 - val_loss: 39.1973\n",
      "Epoch 14/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.7128 - val_loss: 38.6864\n",
      "Epoch 15/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6433 - val_loss: 39.2025\n",
      "Epoch 16/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6752 - val_loss: 39.2837\n",
      "Epoch 17/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6609 - val_loss: 39.3966\n",
      "Epoch 18/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6275 - val_loss: 38.7811\n",
      "Epoch 19/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.4696 - val_loss: 39.2294\n",
      "Epoch 20/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6233 - val_loss: 38.8006\n",
      "Epoch 21/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6348 - val_loss: 38.6302\n",
      "Epoch 22/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6390 - val_loss: 38.9059\n",
      "Epoch 23/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6658 - val_loss: 38.8610\n",
      "Epoch 24/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6262 - val_loss: 39.0005\n",
      "Epoch 25/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.6175 - val_loss: 38.7707\n",
      "Epoch 26/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.5973 - val_loss: 39.1973\n",
      "Epoch 27/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.5599 - val_loss: 38.6519\n",
      "Epoch 28/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.5534 - val_loss: 39.1711\n",
      "Epoch 29/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.5986 - val_loss: 38.9120\n",
      "Epoch 30/150\n",
      "248/248 [==============================] - 1s 5ms/step - loss: 38.5618 - val_loss: 39.2438\n",
      "Epoch 31/150\n",
      "248/248 [==============================] - 1s 4ms/step - loss: 38.4791 - val_loss: 38.9902\n",
      "Epoch 00031: early stopping\n"
     ]
    }
   ],
   "source": [
    "bst = optModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39.63009262084961, 10, 4)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "283/283 [==============================] - 1s 2ms/step - loss: 46.9547 - val_loss: 38.8302\n",
      "Epoch 2/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 41.6621 - val_loss: 37.9488\n",
      "Epoch 3/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 41.1320 - val_loss: 37.7648\n",
      "Epoch 4/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.8469 - val_loss: 37.6603\n",
      "Epoch 5/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.6922 - val_loss: 37.6953\n",
      "Epoch 6/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.6451 - val_loss: 37.6303\n",
      "Epoch 7/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.6070 - val_loss: 37.8697\n",
      "Epoch 8/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.4877 - val_loss: 37.6219\n",
      "Epoch 9/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.4547 - val_loss: 37.5327\n",
      "Epoch 10/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.3841 - val_loss: 37.6237\n",
      "Epoch 11/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.3901 - val_loss: 37.7879\n",
      "Epoch 12/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.2997 - val_loss: 37.5605\n",
      "Epoch 13/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.2834 - val_loss: 37.6030\n",
      "Epoch 14/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.1868 - val_loss: 37.3662\n",
      "Epoch 15/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.2473 - val_loss: 37.5218\n",
      "Epoch 16/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.1765 - val_loss: 37.5575\n",
      "Epoch 17/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.2053 - val_loss: 37.6113\n",
      "Epoch 18/150\n",
      "283/283 [==============================] - 1s 2ms/step - loss: 40.2144 - val_loss: 37.7313\n",
      "Epoch 19/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.1472 - val_loss: 37.4225\n",
      "Epoch 20/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.1240 - val_loss: 37.6104\n",
      "Epoch 21/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.1322 - val_loss: 37.5092\n",
      "Epoch 22/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.1007 - val_loss: 37.5820\n",
      "Epoch 23/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.0629 - val_loss: 37.5777\n",
      "Epoch 24/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.0124 - val_loss: 37.3942\n",
      "Epoch 25/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.1176 - val_loss: 37.7408\n",
      "Epoch 26/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.0598 - val_loss: 37.3241\n",
      "Epoch 27/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.0454 - val_loss: 37.4476\n",
      "Epoch 28/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.0269 - val_loss: 37.4551\n",
      "Epoch 29/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9989 - val_loss: 37.5524\n",
      "Epoch 30/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.0104 - val_loss: 37.4503\n",
      "Epoch 31/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.0077 - val_loss: 37.3943\n",
      "Epoch 32/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9735 - val_loss: 37.6066\n",
      "Epoch 33/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9310 - val_loss: 37.5844\n",
      "Epoch 34/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.0311 - val_loss: 37.4417\n",
      "Epoch 35/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9669 - val_loss: 37.5452\n",
      "Epoch 36/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8838 - val_loss: 37.3104\n",
      "Epoch 37/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9328 - val_loss: 37.5550\n",
      "Epoch 38/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9342 - val_loss: 37.4829\n",
      "Epoch 39/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9415 - val_loss: 37.4792\n",
      "Epoch 40/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9795 - val_loss: 37.6201\n",
      "Epoch 41/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8684 - val_loss: 37.3840\n",
      "Epoch 42/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9539 - val_loss: 37.3984\n",
      "Epoch 43/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9375 - val_loss: 37.6524\n",
      "Epoch 44/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 40.0177 - val_loss: 37.7043\n",
      "Epoch 45/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8582 - val_loss: 37.5832\n",
      "Epoch 46/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8484 - val_loss: 37.4937\n",
      "Epoch 47/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9165 - val_loss: 37.4924\n",
      "Epoch 48/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8373 - val_loss: 37.5359\n",
      "Epoch 49/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.9213 - val_loss: 37.3555\n",
      "Epoch 50/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8496 - val_loss: 37.3309\n",
      "Epoch 51/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8313 - val_loss: 37.5271\n",
      "Epoch 52/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8385 - val_loss: 37.4348\n",
      "Epoch 53/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8789 - val_loss: 37.3310\n",
      "Epoch 54/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8888 - val_loss: 37.7152\n",
      "Epoch 55/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.8408 - val_loss: 37.4321\n",
      "Epoch 56/150\n",
      "283/283 [==============================] - 0s 2ms/step - loss: 39.7969 - val_loss: 37.3996\n",
      "Epoch 00056: early stopping\n"
     ]
    }
   ],
   "source": [
    "a = 10; b = 4\n",
    "bestLoss = 1e12\n",
    "bestA = -1\n",
    "bestB = -1\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(np.shape(X)[1],activation='relu'))\n",
    "\n",
    "for i in range(2+a):\n",
    "    model.add(Dense(np.shape(X)[1],activation='relu'))\n",
    "    model.add(Dropout(1/(2+b)))\n",
    "\n",
    "model.add(Dense(4))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mse')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "model.fit(x=X_train, \n",
    "      y=y_train, \n",
    "      epochs=150, \n",
    "        batch_size=256, \n",
    "        validation_data=(X_test, y_test), \n",
    "        callbacks=[early_stop]\n",
    "      )\n",
    "l = pd.DataFrame(model.history.history).iloc[0,-1]\n",
    "if l < bestLoss:\n",
    "    bestLoss = l\n",
    "    bestA = i\n",
    "    bestB = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.1515654511674389,\n",
       " 0.03586774315460328,\n",
       " -0.00018716812073860112,\n",
       " 0.015574622069261412)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "sklearn.metrics.r2_score(y_test[:,0], pred[:,0]), sklearn.metrics.r2_score(y_test[:,1], pred[:,1]), sklearn.metrics.r2_score(y_test[:,2], pred[:,2]), sklearn.metrics.r2_score(y_test[:,3], pred[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "2mQnSfB0h8Y1",
    "outputId": "ca4b2757-b028-4f8b-acf4-314754ff7b29"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>date</th>\n",
       "      <th>PERMCO</th>\n",
       "      <th>PRC</th>\n",
       "      <th>VOL</th>\n",
       "      <th>CFACPR</th>\n",
       "      <th>sprtrn</th>\n",
       "      <th>AdjP</th>\n",
       "      <th>Ret</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.43750</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.038345</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>-0.014706</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.56250</td>\n",
       "      <td>1711.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>5.708333</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.50000</td>\n",
       "      <td>580.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>-0.007326</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.43750</td>\n",
       "      <td>1406.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.43750</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926891</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>53453</td>\n",
       "      <td>425.25000</td>\n",
       "      <td>8186207.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>425.250000</td>\n",
       "      <td>0.014281</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926892</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>53453</td>\n",
       "      <td>430.94000</td>\n",
       "      <td>10617605.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>430.940000</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926893</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>53453</td>\n",
       "      <td>430.38000</td>\n",
       "      <td>9936455.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>430.380000</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926894</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>53453</td>\n",
       "      <td>414.70001</td>\n",
       "      <td>12563586.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.005781</td>\n",
       "      <td>414.700010</td>\n",
       "      <td>-0.037113</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926895</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>53453</td>\n",
       "      <td>418.32999</td>\n",
       "      <td>10264766.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>418.329990</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35926896 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PERMNO        date  PERMCO        PRC         VOL  CFACPR    sprtrn  \\\n",
       "0          10001  2000-01-04    7953    8.43750      1080.0     1.5 -0.038345   \n",
       "1          10001  2000-01-05    7953    8.56250      1711.0     1.5  0.001922   \n",
       "2          10001  2000-01-06    7953    8.50000       580.0     1.5  0.000956   \n",
       "3          10001  2000-01-07    7953    8.43750      1406.0     1.5  0.027090   \n",
       "4          10001  2000-01-10    7953    8.43750      3390.0     1.5  0.011190   \n",
       "...          ...         ...     ...        ...         ...     ...       ...   \n",
       "35926891   93436  2019-12-24   53453  425.25000   8186207.0     1.0 -0.000195   \n",
       "35926892   93436  2019-12-26   53453  430.94000  10617605.0     1.0  0.005128   \n",
       "35926893   93436  2019-12-27   53453  430.38000   9936455.0     1.0  0.000034   \n",
       "35926894   93436  2019-12-30   53453  414.70001  12563586.0     1.0 -0.005781   \n",
       "35926895   93436  2019-12-31   53453  418.32999  10264766.0     1.0  0.002946   \n",
       "\n",
       "                AdjP       Ret  year  \n",
       "0           5.625000 -0.014706  2000  \n",
       "1           5.708333  0.014706  2000  \n",
       "2           5.666667 -0.007326  2000  \n",
       "3           5.625000 -0.007380  2000  \n",
       "4           5.625000  0.000000  2000  \n",
       "...              ...       ...   ...  \n",
       "35926891  425.250000  0.014281  2019  \n",
       "35926892  430.940000  0.013292  2019  \n",
       "35926893  430.380000 -0.001300  2019  \n",
       "35926894  414.700010 -0.037113  2019  \n",
       "35926895  418.329990  0.008715  2019  \n",
       "\n",
       "[35926896 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2000.to_csv('data2000.csv', index=False)\n",
    "pd.read_csv('data2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "BVx3YUj6i2yD",
    "outputId": "35089deb-f3e8-4c39-8f38-fb3102baab5a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERMNO</th>\n",
       "      <th>date</th>\n",
       "      <th>PERMCO</th>\n",
       "      <th>PRC</th>\n",
       "      <th>VOL</th>\n",
       "      <th>CFACPR</th>\n",
       "      <th>sprtrn</th>\n",
       "      <th>AdjP</th>\n",
       "      <th>Ret</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.43750</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.038345</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>-0.014706</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.56250</td>\n",
       "      <td>1711.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>5.708333</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.50000</td>\n",
       "      <td>580.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>-0.007326</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.43750</td>\n",
       "      <td>1406.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>-0.007380</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10001</td>\n",
       "      <td>2000-01-10</td>\n",
       "      <td>7953</td>\n",
       "      <td>8.43750</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926891</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-24</td>\n",
       "      <td>53453</td>\n",
       "      <td>425.25000</td>\n",
       "      <td>8186207.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>425.250000</td>\n",
       "      <td>0.014281</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926892</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-26</td>\n",
       "      <td>53453</td>\n",
       "      <td>430.94000</td>\n",
       "      <td>10617605.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>430.940000</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926893</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-27</td>\n",
       "      <td>53453</td>\n",
       "      <td>430.38000</td>\n",
       "      <td>9936455.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>430.380000</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926894</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>53453</td>\n",
       "      <td>414.70001</td>\n",
       "      <td>12563586.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.005781</td>\n",
       "      <td>414.700010</td>\n",
       "      <td>-0.037113</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35926895</th>\n",
       "      <td>93436</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>53453</td>\n",
       "      <td>418.32999</td>\n",
       "      <td>10264766.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>418.329990</td>\n",
       "      <td>0.008715</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35400524 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          PERMNO       date  PERMCO        PRC         VOL  CFACPR    sprtrn  \\\n",
       "0          10001 2000-01-04    7953    8.43750      1080.0     1.5 -0.038345   \n",
       "1          10001 2000-01-05    7953    8.56250      1711.0     1.5  0.001922   \n",
       "2          10001 2000-01-06    7953    8.50000       580.0     1.5  0.000956   \n",
       "3          10001 2000-01-07    7953    8.43750      1406.0     1.5  0.027090   \n",
       "4          10001 2000-01-10    7953    8.43750      3390.0     1.5  0.011190   \n",
       "...          ...        ...     ...        ...         ...     ...       ...   \n",
       "35926891   93436 2019-12-24   53453  425.25000   8186207.0     1.0 -0.000195   \n",
       "35926892   93436 2019-12-26   53453  430.94000  10617605.0     1.0  0.005128   \n",
       "35926893   93436 2019-12-27   53453  430.38000   9936455.0     1.0  0.000034   \n",
       "35926894   93436 2019-12-30   53453  414.70001  12563586.0     1.0 -0.005781   \n",
       "35926895   93436 2019-12-31   53453  418.32999  10264766.0     1.0  0.002946   \n",
       "\n",
       "                AdjP       Ret  year  \n",
       "0           5.625000 -0.014706  2000  \n",
       "1           5.708333  0.014706  2000  \n",
       "2           5.666667 -0.007326  2000  \n",
       "3           5.625000 -0.007380  2000  \n",
       "4           5.625000  0.000000  2000  \n",
       "...              ...       ...   ...  \n",
       "35926891  425.250000  0.014281  2019  \n",
       "35926892  430.940000  0.013292  2019  \n",
       "35926893  430.380000 -0.001300  2019  \n",
       "35926894  414.700010 -0.037113  2019  \n",
       "35926895  418.329990  0.008715  2019  \n",
       "\n",
       "[35400524 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2000.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "id": "AyGoIuFVsGe1",
    "outputId": "f2b88541-9a57-4ba6-dcb8-272fffdcabd2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10001</th>\n",
       "      <th>10002</th>\n",
       "      <th>10009</th>\n",
       "      <th>10012</th>\n",
       "      <th>10016</th>\n",
       "      <th>10019</th>\n",
       "      <th>10025</th>\n",
       "      <th>10026</th>\n",
       "      <th>10028</th>\n",
       "      <th>10032</th>\n",
       "      <th>...</th>\n",
       "      <th>93427</th>\n",
       "      <th>93428</th>\n",
       "      <th>93429</th>\n",
       "      <th>93430</th>\n",
       "      <th>93431</th>\n",
       "      <th>93432</th>\n",
       "      <th>93433</th>\n",
       "      <th>93434</th>\n",
       "      <th>93435</th>\n",
       "      <th>93436</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>-0.014706</td>\n",
       "      <td>0.040822</td>\n",
       "      <td>-0.144250</td>\n",
       "      <td>-0.089856</td>\n",
       "      <td>-0.086433</td>\n",
       "      <td>0.031351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.088107</td>\n",
       "      <td>-0.064539</td>\n",
       "      <td>-0.027098</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>0.014706</td>\n",
       "      <td>-0.040822</td>\n",
       "      <td>0.072722</td>\n",
       "      <td>0.089856</td>\n",
       "      <td>0.034133</td>\n",
       "      <td>-0.050644</td>\n",
       "      <td>-0.022990</td>\n",
       "      <td>0.057524</td>\n",
       "      <td>0.064539</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>-0.007326</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>-0.047188</td>\n",
       "      <td>-0.064539</td>\n",
       "      <td>-0.044604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.028836</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>-0.007380</td>\n",
       "      <td>0.059423</td>\n",
       "      <td>0.024898</td>\n",
       "      <td>0.008299</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>-0.095310</td>\n",
       "      <td>0.071826</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>-0.032435</td>\n",
       "      <td>0.007391</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.019418</td>\n",
       "      <td>-0.050431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>-0.014389</td>\n",
       "      <td>-0.009950</td>\n",
       "      <td>-0.034540</td>\n",
       "      <td>0.032435</td>\n",
       "      <td>0.024729</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.117783</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.003764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005648</td>\n",
       "      <td>-0.422272</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>-0.019293</td>\n",
       "      <td>-0.143101</td>\n",
       "      <td>-0.013668</td>\n",
       "      <td>0.072149</td>\n",
       "      <td>0.014281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.117783</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>-0.002882</td>\n",
       "      <td>-0.002579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009371</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>-0.422272</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>-0.019293</td>\n",
       "      <td>-0.143101</td>\n",
       "      <td>-0.037388</td>\n",
       "      <td>0.072149</td>\n",
       "      <td>0.013292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.117783</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>-0.013950</td>\n",
       "      <td>-0.006736</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001766</td>\n",
       "      <td>-0.422272</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>-0.019293</td>\n",
       "      <td>-0.143101</td>\n",
       "      <td>-0.058841</td>\n",
       "      <td>0.072149</td>\n",
       "      <td>-0.001300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.117783</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.003758</td>\n",
       "      <td>-0.019803</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003444</td>\n",
       "      <td>-0.422272</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>-0.019293</td>\n",
       "      <td>-0.143101</td>\n",
       "      <td>0.005038</td>\n",
       "      <td>0.072149</td>\n",
       "      <td>-0.037113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.117783</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.008968</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>-0.003633</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006270</td>\n",
       "      <td>-0.422272</td>\n",
       "      <td>0.004152</td>\n",
       "      <td>-0.019293</td>\n",
       "      <td>-0.143101</td>\n",
       "      <td>0.053803</td>\n",
       "      <td>0.072149</td>\n",
       "      <td>0.008715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5030 rows × 18371 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               10001     10002     10009     10012     10016     10019  \\\n",
       "date                                                                     \n",
       "2000-01-04 -0.014706  0.040822 -0.144250 -0.089856 -0.086433  0.031351   \n",
       "2000-01-05  0.014706 -0.040822  0.072722  0.089856  0.034133 -0.050644   \n",
       "2000-01-06 -0.007326  0.020619 -0.047188 -0.064539 -0.044604  0.000000   \n",
       "2000-01-07 -0.007380  0.059423  0.024898  0.008299  0.010471 -0.095310   \n",
       "2000-01-10  0.000000 -0.019418 -0.050431  0.000000  0.003466 -0.014389   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2019-12-24  0.000000  0.020340  0.000000 -0.117783  0.006181  0.000000   \n",
       "2019-12-26  0.000000  0.020340  0.000000 -0.117783  0.006181  0.000000   \n",
       "2019-12-27  0.000000  0.020340  0.000000 -0.117783  0.006181  0.000000   \n",
       "2019-12-30  0.000000  0.020340  0.000000 -0.117783  0.006181  0.000000   \n",
       "2019-12-31  0.000000  0.020340  0.000000 -0.117783  0.006181  0.000000   \n",
       "\n",
       "               10025     10026     10028     10032  ...     93427  93428  \\\n",
       "date                                                ...                    \n",
       "2000-01-04  0.000000 -0.088107 -0.064539 -0.027098  ...       NaN    NaN   \n",
       "2000-01-05 -0.022990  0.057524  0.064539 -0.000743  ...       NaN    NaN   \n",
       "2000-01-06 -0.028836  0.003101 -0.021053  0.001485  ...       NaN    NaN   \n",
       "2000-01-07  0.071826  0.003091 -0.032435  0.007391  ...       NaN    NaN   \n",
       "2000-01-10 -0.009950 -0.034540  0.032435  0.024729  ...       NaN    NaN   \n",
       "...              ...       ...       ...       ...  ...       ...    ...   \n",
       "2019-12-24 -0.000455 -0.003764  0.000000  0.001933  ...  0.005882    0.0   \n",
       "2019-12-26 -0.000455  0.003066 -0.002882 -0.002579  ...  0.009371    0.0   \n",
       "2019-12-27 -0.000455  0.002414 -0.013950 -0.006736  ... -0.012772    0.0   \n",
       "2019-12-30 -0.000455 -0.003758 -0.019803  0.003633  ...  0.005868    0.0   \n",
       "2019-12-31 -0.000455 -0.008968  0.007435 -0.003633  ... -0.001695    0.0   \n",
       "\n",
       "               93429     93430     93431     93432     93433     93434  \\\n",
       "date                                                                     \n",
       "2000-01-04       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2000-01-05       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2000-01-06       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2000-01-07       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2000-01-10       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2019-12-24  0.005648 -0.422272  0.004152 -0.019293 -0.143101 -0.013668   \n",
       "2019-12-26  0.000672 -0.422272  0.004152 -0.019293 -0.143101 -0.037388   \n",
       "2019-12-27 -0.001766 -0.422272  0.004152 -0.019293 -0.143101 -0.058841   \n",
       "2019-12-30  0.003444 -0.422272  0.004152 -0.019293 -0.143101  0.005038   \n",
       "2019-12-31  0.006270 -0.422272  0.004152 -0.019293 -0.143101  0.053803   \n",
       "\n",
       "               93435     93436  \n",
       "date                            \n",
       "2000-01-04       NaN       NaN  \n",
       "2000-01-05       NaN       NaN  \n",
       "2000-01-06       NaN       NaN  \n",
       "2000-01-07       NaN       NaN  \n",
       "2000-01-10       NaN       NaN  \n",
       "...              ...       ...  \n",
       "2019-12-24  0.072149  0.014281  \n",
       "2019-12-26  0.072149  0.013292  \n",
       "2019-12-27  0.072149 -0.001300  \n",
       "2019-12-30  0.072149 -0.037113  \n",
       "2019-12-31  0.072149  0.008715  \n",
       "\n",
       "[5030 rows x 18371 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rets = data2000.pivot(index='date', columns='PERMNO', values='Ret')\n",
    "rets = rets.interpolate()\n",
    "rets.to_csv('rets.csv')\n",
    "pd.read_csv('rets.csv').set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "MVSK NN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
